{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec Subsampling Frequent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the training set is discarded with the probability computed by the form: $P(w_i) = 1 - \\sqrt{\\frac{threshold}{freq(w_i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_func_factory(threshold):\n",
    "    return lambda freq: max(0, 1 - math.sqrt(threshold / freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [1e-5, 1e-4, 1e-3]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    prob_fn = prob_func_factory(threshold)\n",
    "    freqs = range(1, 1000000, 2500)\n",
    "    freqs_sum = float(sum(freqs))\n",
    "    freqs = [freq / freqs_sum for freq in freqs]\n",
    "    probs = [prob_fn(freq) for freq in freqs]\n",
    "    \n",
    "    plt.semilogx(freqs, probs, label=threshold)\n",
    "    \n",
    "plt.ylabel('Prob of word being removed')\n",
    "plt.xlabel('Occurences of word')\n",
    "plt.legend(loc='upper left'); plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula was chosen because it aggresively subsamples words whose frequency is higher than the set threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "Code below converts text data to unified format with one sentence per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import numpy\n",
    "import string\n",
    "import codecs\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presidential Speeches Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "CORPORA_DIR = './corpora/presidential-transcripts-raw/'\n",
    "CLEAN_FILE_NAME = './corpora/presidential-clean.txt'\n",
    "\n",
    "# regexps\n",
    "with codecs.open(CLEAN_FILE_NAME, 'w', encoding='ascii', errors='ignore') as out:\n",
    "    for root_dir, dirs, files in os.walk(CORPORA_DIR):\n",
    "        print 'Exploring: ', root_dir\n",
    "        for file_name in files:\n",
    "            if not re.match('\\.DS_Store', file_name):\n",
    "                file_path = os.path.join(root_dir, file_name)\n",
    "                with open(file_path) as fd:\n",
    "                    # read entire file\n",
    "                    raw_content = [line.decode('ascii', errors='replace') for line in fd.readlines()]\n",
    "\n",
    "                    # translate to lowercase\n",
    "                    lower_contents = [text.lower() for text in raw_content]\n",
    "\n",
    "                    # tokenizer speech into sentences\n",
    "                    for fragment in lower_contents:\n",
    "                        [out.write(line.strip() + '\\n') for line in nltk.tokenize.sent_tokenize(fragment)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airbnb Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring:  ./corpora/reviews/\n"
     ]
    }
   ],
   "source": [
    "# filepaths\n",
    "CORPORA_DIR = './corpora/reviews/'\n",
    "CLEAN_FILE_NAME = './corpora/reviews-clean.txt'\n",
    "\n",
    "# regexps\n",
    "with codecs.open(CLEAN_FILE_NAME, 'w', encoding='ascii', errors='ignore') as out:\n",
    "    for root_dir, dirs, files in os.walk(CORPORA_DIR):\n",
    "        print 'Exploring: ', root_dir\n",
    "        for file_name in files:\n",
    "            if not re.match('\\.DS_Store', file_name):\n",
    "                file_path = os.path.join(root_dir, file_name)\n",
    "                \n",
    "                with open(file_path) as fd:\n",
    "                    csv_reader = csv.reader(fd)\n",
    "                    next(csv_reader, None)\n",
    "                    # read and decodeentire file\n",
    "                    raw_content = [row[-1].decode('utf8', errors='replace') for row in csv_reader]\n",
    "                    \n",
    "                    # remove new lines\n",
    "                    clean_content = [re.sub(r'[\\n\\r ]+', ' ', line) for line in raw_content]\n",
    "\n",
    "                    # translate to lowercase\n",
    "                    lower_contents = [text.lower() for text in raw_content]\n",
    "\n",
    "                    # tokenizer speech into sentences\n",
    "                    for fragment in lower_contents:\n",
    "                        [out.write(line.strip() + '\\n') for line in nltk.tokenize.sent_tokenize(fragment)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tensorflow seq2seq using raw_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toy_data_generator(vocab_size, data_size, max_seq_length, reserved_digits=3):\n",
    "    for _ in xrange(data_size):\n",
    "        seq_length = np.random.randint(max_seq_length) + 1\n",
    "        yield [np.random.randint(vocab_size-reserved_digits)+reserved_digits for _ in xrange(seq_length)]\n",
    "        \n",
    "def batchify_data(data_generator, batch_size):\n",
    "    \"\"\" Split dataset (generator) into batches \"\"\"\n",
    "    if isinstance(data_generator, list):\n",
    "        for ix in xrange(0, len(data_generator), batch_size):\n",
    "            buff = data_generator[ix:ix+batch_size]\n",
    "            yield buff\n",
    "    else:\n",
    "        while data_generator:\n",
    "            buff = []\n",
    "            for ix in xrange(0, batch_size):\n",
    "                buff.append(next(data_generator))\n",
    "            yield buff\n",
    "            \n",
    "def pad_data(data_arr, append_pre=[], append_suf=[], max_length=None):\n",
    "    data_arr = [append_pre + row + append_suf for row in data_arr]\n",
    "    lengths = [len(row) for row in data_arr]\n",
    "    max_len = max(lengths) if not max_length else max_length\n",
    "    return np.array([row+[0]*(max_len-length) for row, length in zip(data_arr, lengths)]), lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define constant parameters\n",
    "VOCAB_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LENGTH = 5\n",
    "\n",
    "EMB_SIZE = 20\n",
    "ENC_HIDDEN_UNITS = 20\n",
    "DEC_HIDDEN_UNITS = ENC_HIDDEN_UNITS\n",
    "\n",
    "EOS_TOKEN = 2\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# define placeholders for data\n",
    "enc_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "enc_inputs_len = tf.placeholder(shape=(None), dtype=tf.int32, name='encoder_inputs_len')\n",
    "dec_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "dec_targets_len = tf.placeholder(shape=(None), dtype=tf.int32, name='decoder_targets_len')\n",
    "\n",
    "# define embeddings and lookup\n",
    "embeddings_src = tf.Variable(tf.random_uniform([VOCAB_SIZE_SRC, EMB_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "embeddings_trg = tf.Variable(tf.random_uniform([VOCAB_SIZE_TRG, EMB_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "enc_inputs_emb = tf.nn.embedding_lookup(embeddings_src, enc_inputs)\n",
    "\n",
    "# define encoder\n",
    "enc_cell = tf.contrib.rnn.GRUCell(ENC_HIDDEN_UNITS)\n",
    "_, enc_final_state = tf.nn.dynamic_rnn(\n",
    "    enc_cell, enc_inputs_emb, dtype=tf.float32, time_major=True, scope='encoder_cell')\n",
    "\n",
    "# define decoder\n",
    "dec_cell = tf.contrib.rnn.GRUCell(DEC_HIDDEN_UNITS)\n",
    "\n",
    "dec_smax_W = tf.Variable(tf.random_uniform([DEC_HIDDEN_UNITS, VOCAB_SIZE_TRG], -1.0, 1.0), dtype=tf.float32)\n",
    "dec_smax_b = tf.Variable(tf.zeros([VOCAB_SIZE_TRG]), dtype=tf.float32)\n",
    "\n",
    "eos_slice = tf.fill([BATCH_SIZE], EOS_TOKEN, name='EOS')\n",
    "pad_slice = tf.fill([BATCH_SIZE], PAD_TOKEN, name='PAD')\n",
    "\n",
    "eos_slice_emb = tf.nn.embedding_lookup(embeddings_src, eos_slice)\n",
    "pad_slice_emb = tf.nn.embedding_lookup(embeddings_src, pad_slice)\n",
    "\n",
    "# loop transition function (defines inputs of step t given outputs of step t-1)\n",
    "# (time, prev_cell_output, prev_cell_state, prev_loop_state) -> (elements_finished, input, cell_state, output, loop_state)\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    if prev_state is None:\n",
    "        init_elements_finished = (0 >= dec_targets_len)\n",
    "        init_input = eos_slice_emb\n",
    "        init_cell_state = enc_final_state\n",
    "        init_cell_output = None\n",
    "        init_loop_state = None\n",
    "        return (init_elements_finished, init_input, init_cell_state, init_cell_output, init_loop_state)\n",
    "    else:\n",
    "        def get_next_input():\n",
    "            output_logits = tf.add(tf.matmul(prev_output, dec_smax_W), dec_smax_b)\n",
    "            pred = tf.argmax(output_logits, axis=1)\n",
    "            next_input = tf.nn.embedding_lookup(embeddings_trg, pred)\n",
    "            return next_input\n",
    "\n",
    "        step_elements_finished = (time >= dec_targets_len)\n",
    "        step_finished = tf.reduce_all(step_elements_finished)\n",
    "        step_input = tf.cond(step_finished, lambda: pad_slice_emb, get_next_input)\n",
    "        step_state = prev_state\n",
    "        step_output = prev_output\n",
    "        step_loop_state = None\n",
    "        return (step_elements_finished, step_input, step_state, step_output, step_loop_state)\n",
    "\n",
    "dec_outputs_ta, dec_final_state, _ = tf.nn.raw_rnn(\n",
    "    dec_cell, loop_fn)\n",
    "dec_outputs = dec_outputs_ta.stack()\n",
    "\n",
    "dec_max_steps, dec_batch_size, dec_dim = tf.unstack(tf.shape(dec_outputs))\n",
    "dec_outputs_flat = tf.reshape(dec_outputs, (-1, dec_dim))\n",
    "dec_logits_flat = tf.add(tf.matmul(dec_outputs_flat, dec_smax_W), dec_smax_b)\n",
    "dec_logits = tf.reshape(dec_logits_flat, (dec_max_steps, dec_batch_size, VOCAB_SIZE_TRG))\n",
    "dec_preds = tf.argmax(dec_logits, 2)\n",
    "\n",
    "# define loss function and optimizer\n",
    "stepwise_cent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(dec_targets, depth=VOCAB_SIZE_TRG, dtype=tf.float32),\n",
    "    logits=dec_logits)\n",
    "loss = tf.reduce_mean(stepwise_cent)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.35056734085\n",
      "  sample 1:\n",
      "    input     > [9 5 6 9 6 5 6 7 7 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 9 8 9 8 9 9 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 5 4 7 6 3 4 4 7 5 7 6 8 3 9 6 2 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 9 8 9 4 5 9 5 9 5 9 5 6 6 6 6 0 0 0 0 0 0 0]\n",
      "batch 100\n",
      "  minibatch loss: 2.06731200218\n",
      "  sample 1:\n",
      "    input     > [4 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 4 7 8 9 7 7 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 5 8 3 3 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 7 3 6 7 9 9 9 9 4 4 3 7 3 8 9 5 4 2 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 5 7 5 7 9 5 7 9 5 7 9 9 4 3 0 0 0 0 0]\n",
      "batch 200\n",
      "  minibatch loss: 1.94717085361\n",
      "  sample 1:\n",
      "    input     > [4 9 5 8 7 4 6 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 4 4 5 8 9 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 4 3 7 8 9 3 8 6 5 6 5 8 8 6 8 6 5 8 2 0 0 0 0 0 0]\n",
      "    predicted > [5 6 6 6 6 9 8 8 9 8 9 5 8 9 8 9 8 9 2 2 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 9 7 6 7 6 4 5 7 8 6 8 3 5 7 9 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 6 6 6 9 5 8 9 8 9 8 9 8 9 3 2 2 0 0 0 0 0 0 0 0]\n",
      "batch 300\n",
      "  minibatch loss: 1.84659981728\n",
      "  sample 1:\n",
      "    input     > [9 4 8 4 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 9 3 4 9 6 5 8 7 9 8 5 3 3 7 5 8 8 9 4 2 0 0 0 0]\n",
      "    predicted > [7 9 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 5 4 5 3 9 3 3 7 8 5 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 9 5 5 6 6 6 6 6 6 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 400\n",
      "  minibatch loss: 1.7251086235\n",
      "  sample 1:\n",
      "    input     > [8 7 6 4 4 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 8 7 9 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 9 7 9 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 500\n",
      "  minibatch loss: 1.66514825821\n",
      "  sample 1:\n",
      "    input     > [8 9 4 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 6 4 7 4 4 6 8 3 3 3 8 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 4 4 4 4 4 4 4 4 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 5 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 600\n",
      "  minibatch loss: 1.58640372753\n",
      "  sample 1:\n",
      "    input     > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 7 4 6 4 4 5 6 4 3 3 7 6 5 5 5 6 3 6 8 6 9 8 2 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 2 2 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 9 4 8 9 8 4 3 9 7 6 8 8 3 7 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0]\n",
      "batch 700\n",
      "  minibatch loss: 1.49040853977\n",
      "  sample 1:\n",
      "    input     > [8 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 4 7 4 6 9 7 4 7 4 7 6 7 8 5 3 3 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 6 7 7 7 7 7 7 7 7 7 7 6 2 2 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 4 8 6 9 5 3 6 6 4 6 5 6 4 5 4 8 6 5 3 3 2 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 0 0 0 0]\n",
      "batch 800\n",
      "  minibatch loss: 1.52136695385\n",
      "  sample 1:\n",
      "    input     > [3 6 7 9 8 5 9 4 8 4 8 4 5 3 6 7 8 5 6 7 2 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 2 2 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 4 8 9 5 7 9 9 8 7 3 4 7 8 8 7 8 9 9 4 5 2 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 2 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 5 7 3 6 4 3 8 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 900\n",
      "  minibatch loss: 1.47104239464\n",
      "  sample 1:\n",
      "    input     > [7 6 8 9 8 4 7 6 4 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 8 4 6 8 6 4 9 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 3 9 9 7 9 3 9 3 4 8 7 5 7 6 9 7 8 2 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 2 0 0 0 0 0 0]\n",
      "batch 1000\n",
      "  minibatch loss: 1.40793859959\n",
      "  sample 1:\n",
      "    input     > [4 3 9 7 9 7 7 9 6 5 3 6 5 8 9 6 5 9 2 0 0 0 0 0 0 0]\n",
      "    predicted > [7 9 7 9 9 6 9 9 6 9 9 9 9 9 6 6 6 2 2 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 7 4 7 8 9 9 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 4 6 6 7 7 9 4 4 5 3 5 8 7 7 8 5 8 2 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 0 0 0 0 0 0 0]\n",
      "batch 1100\n",
      "  minibatch loss: 1.29577720165\n",
      "  sample 1:\n",
      "    input     > [9 9 7 5 7 4 9 8 3 6 5 3 3 5 7 5 8 8 2 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 9 8 9 8 9 8 2 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 9 7 9 7 6 7 8 4 9 9 5 5 7 6 4 9 5 8 9 5 7 5 8 2 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 8 9 6 2 0]\n",
      "  sample 3:\n",
      "    input     > [5 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1200\n",
      "  minibatch loss: 1.29744160175\n",
      "  sample 1:\n",
      "    input     > [8 8 6 7 6 8 3 8 3 6 4 3 9 9 3 8 5 7 8 5 4 5 5 7 2]\n",
      "    predicted > [7 7 7 7 3 3 3 3 6 3 3 6 7 7 7 7 7 7 7 7 7 7 7 6 2]\n",
      "  sample 2:\n",
      "    input     > [5 9 7 9 7 4 8 5 8 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 5 8 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1300\n",
      "  minibatch loss: 1.24666798115\n",
      "  sample 1:\n",
      "    input     > [3 9 9 5 5 8 7 5 7 4 9 8 4 4 8 5 7 7 8 5 2 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 8 8 8 8 5 5 5 5 5 5 5 5 5 5 5 5 2 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 5 6 3 9 4 3 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 7 9 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1400\n",
      "  minibatch loss: 1.31937515736\n",
      "  sample 1:\n",
      "    input     > [4 4 3 8 9 4 6 5 5 3 9 4 6 7 9 9 5 4 9 3 5 8 3 6 3 2]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 9 9 9 9 9 9 9 9 9 9 9 9 9 6 6 2 2]\n",
      "  sample 2:\n",
      "    input     > [5 6 8 9 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 4 8 5 4 9 4 8 9 6 9 6 9 3 9 7 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 9 9 9 9 9 9 9 9 9 8 2 2 0 0 0 0 0 0 0 0 0]\n",
      "batch 1500\n",
      "  minibatch loss: 1.24324178696\n",
      "  sample 1:\n",
      "    input     > [5 8 9 5 7 3 6 3 8 6 5 6 8 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 8 8 8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 9 5 4 9 5 4 8 8 5 8 9 9 6 4 3 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 9 9 9 9 2 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 3 7 6 3 7 4 8 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 6 6 6 6 7 3 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1600\n",
      "  minibatch loss: 1.10721468925\n",
      "  sample 1:\n",
      "    input     > [6 6 3 5 5 5 5 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 6 9 4 6 5 7 6 8 4 9 9 4 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 5 5 5 5 5 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 5 4 4 8 6 7 5 3 6 4 9 8 5 5 6 9 5 3 2 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1700\n",
      "  minibatch loss: 0.984771430492\n",
      "  sample 1:\n",
      "    input     > [6 9 7 6 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 7 7 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 8 7 6 4 4 3 6 4 3 8 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1800\n",
      "  minibatch loss: 1.07512998581\n",
      "  sample 1:\n",
      "    input     > [4 6 9 5 4 4 3 5 9 7 4 3 3 5 7 2 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 4 3 3 6 4 4 8 6 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 3 3 3 3 7 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 7 7 3 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 1900\n",
      "  minibatch loss: 1.11294412613\n",
      "  sample 1:\n",
      "    input     > [9 9 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 8 8 8 5 6 7 8 8 3 3 9 4 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 2000\n",
      "  minibatch loss: 1.1031730175\n",
      "  sample 1:\n",
      "    input     > [6 8 3 9 3 6 5 5 8 5 8 8 9 7 4 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 5 5 5 9 5 9 9 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 7 5 7 5 7 9 9 7 9 6 8 4 3 5 5 5 9 8 5 6 7 8 2 0]\n",
      "    predicted > [7 7 7 7 7 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 0]\n",
      "  sample 3:\n",
      "    input     > [8 6 4 5 6 6 4 8 6 6 6 7 6 5 3 6 8 8 7 6 3 7 2 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 7 6 7 7 7 7 7 7 7 7 7 2 0 0]\n",
      "batch 2100\n",
      "  minibatch loss: 1.06469249725\n",
      "  sample 1:\n",
      "    input     > [7 6 8 9 9 6 5 7 5 7 3 6 3 3 7 3 5 5 9 3 3 3 8 2 0]\n",
      "    predicted > [6 6 6 3 6 3 3 6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 0]\n",
      "  sample 2:\n",
      "    input     > [6 3 8 7 6 6 3 6 4 9 3 8 3 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 6 7 7 7 4 7 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 6 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 2200\n",
      "  minibatch loss: 1.03910839558\n",
      "  sample 1:\n",
      "    input     > [8 8 3 6 6 7 7 4 5 4 9 3 6 4 9 6 6 4 3 7 4 7 4 2 0 0]\n",
      "    predicted > [4 4 4 4 4 4 6 7 6 7 7 6 7 7 7 7 7 7 7 7 7 7 7 2 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 5 6 5 5 6 9 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 5 3 8 9 8 5 5 8 3 9 4 7 4 5 9 9 7 7 2 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 8 8 5 8 8 5 5 5 5 5 2 0 0 0 0 0 0]\n",
      "batch 2300\n",
      "  minibatch loss: 1.06231796741\n",
      "  sample 1:\n",
      "    input     > [5 6 6 4 4 8 3 4 8 7 7 3 6 3 6 4 9 3 8 2 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 7 7 5 7 3 6 3 5 8 7 6 9 9 6 3 5 9 3 5 8 2 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 7 2 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 5 9 6 7 6 5 4 6 4 3 4 9 9 7 5 6 5 8 6 6 8 7 2 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 5 6 5 6 5 6 5 5 5 5 5 5 5 5 5 6 2 0 0]\n",
      "batch 2400\n",
      "  minibatch loss: 1.02508807182\n",
      "  sample 1:\n",
      "    input     > [5 7 5 3 9 3 3 4 9 6 4 9 7 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 5 5 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 8 3 9 7 9 8 5 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 3 3 3 3 3 3 3 3 3 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 9 4 5 4 8 3 9 4 8 5 4 3 7 8 6 7 5 7 2 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 7 7 7 2 0 0 0 0 0 0]\n",
      "batch 2500\n",
      "  minibatch loss: 0.957047998905\n",
      "  sample 1:\n",
      "    input     > [3 7 6 7 6 9 4 3 9 3 5 9 7 9 8 5 5 7 6 5 3 7 7 3 4 2]\n",
      "    predicted > [7 7 7 3 3 6 6 3 6 6 3 6 3 6 3 6 7 7 7 7 7 7 7 7 7 2]\n",
      "  sample 2:\n",
      "    input     > [3 4 5 6 8 5 4 4 5 3 4 3 7 9 5 7 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 3 6 7 9 4 4 3 9 6 6 8 6 8 6 9 6 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 8 8 6 8 8 8 8 8 6 2 0 0 0 0 0 0 0 0]\n",
      "batch 2600\n",
      "  minibatch loss: 0.928827524185\n",
      "  sample 1:\n",
      "    input     > [6 9 4 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 4 7 9 7 9 6 9 6 4 5 3 9 8 5 3 3 8 4 7 5 4 9 2]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 2]\n",
      "batch 2700\n",
      "  minibatch loss: 1.00975263119\n",
      "  sample 1:\n",
      "    input     > [7 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 8 3 8 8 8 7 9 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 3 5 3 4 4 3 3 5 5 8 5 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 5 5 5 5 5 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 2800\n",
      "  minibatch loss: 1.00874018669\n",
      "  sample 1:\n",
      "    input     > [9 7 6 6 5 5 3 9 3 6 6 9 5 4 6 6 3 8 4 8 3 2 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 5 5 3 5 5 3 3 3 3 2 2 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 3 6 6 5 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 3 9 5 6 7 9 6 3 7 4 6 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 6 6 6 6 6 6 6 6 7 7 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 2900\n",
      "  minibatch loss: 0.883655786514\n",
      "  sample 1:\n",
      "    input     > [5 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 5 7 8 6 3 7 7 9 7 6 8 5 4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 3 7 7 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 3000\n",
      "  minibatch loss: 1.12066364288\n",
      "  sample 1:\n",
      "    input     > [9 8 7 3 6 5 7 4 7 5 7 9 3 3 6 8 9 4 6 9 5 2 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 9 7 7 9 7 6 7 4 4 4 4 7 4 7 4 2 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 4 7 3 3 7 4 5 3 5 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 3 3 3 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 8 6 7 4 3 8 5 3 9 4 9 7 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 9 4 4 4 7 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 3100\n",
      "  minibatch loss: 0.961293280125\n",
      "  sample 1:\n",
      "    input     > [6 6 5 5 4 7 4 8 6 8 3 6 4 8 6 3 4 7 6 7 3 2 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 3 3 3 3 2 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 5 7 3 9 9 7 3 8 3 9 6 4 6 6 6 4 3 8 7 4 3 2 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 6 7 3 9 8 4 7 6 9 5 6 6 6 4 7 9 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 0 0 0 0 0 0 0 0]\n",
      "batch 3200\n",
      "  minibatch loss: 0.875748693943\n",
      "  sample 1:\n",
      "    input     > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 9 3 9 7 7 6 4 4 4 4 6 4 9 3 2 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 3 3 3 4 4 4 4 4 4 4 2 2 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 8 6 6 9 4 6 4 9 3 9 7 9 6 7 7 5 6 5 6 6 6 5 2 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 6 2 0 0]\n",
      "batch 3300\n",
      "  minibatch loss: 0.916333317757\n",
      "  sample 1:\n",
      "    input     > [8 7 3 4 7 3 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 4 7 4 3 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 8 3 7 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3400\n",
      "  minibatch loss: 1.0804681778\n",
      "  sample 1:\n",
      "    input     > [3 7 6 8 8 9 5 6 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 9 7 6 4 3 5 5 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 6 6 6 6 7 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 8 4 6 7 8 7 7 5 5 8 3 7 4 5 8 4 4 6 2 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 0 0 0 0]\n",
      "batch 3500\n",
      "  minibatch loss: 0.97028952837\n",
      "  sample 1:\n",
      "    input     > [4 9 4 9 6 6 3 8 3 7 7 6 3 5 9 4 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 9 6 6 4 6 4 6 4 4 4 3 4 6 2 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 3 4 9 6 9 9 4 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 6 7 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 3600\n",
      "  minibatch loss: 0.796277463436\n",
      "  sample 1:\n",
      "    input     > [3 8 4 6 7 3 5 7 8 7 5 7 8 5 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 7 7 7 7 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 8 6 3 3 5 4 6 7 9 5 5 6 9 6 3 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 7 3 3 7 5 4 4 3 8 9 5 7 6 4 6 5 9 5 6 4 2 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 0 0 0 0]\n",
      "batch 3700\n",
      "  minibatch loss: 1.05658173561\n",
      "  sample 1:\n",
      "    input     > [7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 6 4 4 9 3 9 9 4 4 4 3 4 4 7 5 7 3 9 8 4 4 8 2 0 0]\n",
      "    predicted > [4 4 4 9 4 4 4 4 4 4 4 4 9 9 9 9 9 9 9 9 9 9 2 2 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 6 3 5 3 8 9 6 5 7 9 8 5 3 7 3 7 8 4 8 9 2 0 0 0 0]\n",
      "    predicted > [6 3 3 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 2 0 0 0 0]\n",
      "batch 3800\n",
      "  minibatch loss: 0.937764525414\n",
      "  sample 1:\n",
      "    input     > [7 8 5 3 3 4 4 5 5 7 6 8 5 3 5 3 8 6 6 9 3 4 6 2 0]\n",
      "    predicted > [5 8 5 3 5 3 3 3 5 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 0]\n",
      "  sample 2:\n",
      "    input     > [8 5 3 3 8 6 5 6 4 7 9 8 6 9 8 4 9 8 4 6 2 0 0 0 0]\n",
      "    predicted > [8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 3 3 4 5 4 8 5 9 7 8 3 4 5 6 9 6 3 2 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0]\n",
      "batch 3900\n",
      "  minibatch loss: 0.751308202744\n",
      "  sample 1:\n",
      "    input     > [3 3 7 3 5 8 9 9 3 7 9 8 3 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 7 9 6 9 9 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 9 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 4 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4000\n",
      "  minibatch loss: 0.862003445625\n",
      "  sample 1:\n",
      "    input     > [9 7 7 7 8 9 5 9 5 8 7 8 9 3 8 9 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 6 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 7 8 7 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4100\n",
      "  minibatch loss: 0.976456701756\n",
      "  sample 1:\n",
      "    input     > [8 3 8 8 6 4 9 6 7 5 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 6 6 6 6 6 8 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 3 9 3 6 8 6 7 3 4 7 6 4 6 4 4 4 8 6 2 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 9 7 6 4 4 8 4 5 7 3 7 6 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 7 7 7 7 7 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4200\n",
      "  minibatch loss: 0.831892549992\n",
      "  sample 1:\n",
      "    input     > [4 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 4 4 7 4 3 6 8 8 9 3 6 5 9 9 3 7 5 5 2 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 4 6 9 3 5 9 7 3 9 5 8 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 4 4 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4300\n",
      "  minibatch loss: 0.949724912643\n",
      "  sample 1:\n",
      "    input     > [7 7 4 3 3 6 3 6 3 8 7 5 3 4 7 8 5 5 3 2 0 0 0 0 0]\n",
      "    predicted > [7 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 2 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 3 8 3 6 3 5 7 3 5 4 9 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 3 3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 7 7 5 4 4 5 7 5 9 8 3 4 4 9 7 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 5 5 5 5 5 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0]\n",
      "batch 4400\n",
      "  minibatch loss: 0.781534731388\n",
      "  sample 1:\n",
      "    input     > [6 9 3 5 4 8 4 7 8 9 4 8 8 6 9 3 9 4 5 2 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 8 8 9 8 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 5 8 3 6 3 4 7 4 8 4 3 4 8 9 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 3 3 3 8 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 7 9 8 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 7 7 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4500\n",
      "  minibatch loss: 0.77523624897\n",
      "  sample 1:\n",
      "    input     > [7 8 8 9 9 3 5 4 9 3 9 7 3 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 5 5 8 8 9 7 4 8 7 7 3 4 3 9 6 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 8 8 8 8 8 8 8 8 7 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4600\n",
      "  minibatch loss: 0.916509509087\n",
      "  sample 1:\n",
      "    input     > [4 9 5 6 5 6 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 6 9 9 9 9 9 5 7 6 9 3 5 7 6 7 7 5 4 7 9 3 9 6 2]\n",
      "    predicted > [9 9 9 9 9 9 9 9 9 9 9 9 9 7 9 7 9 7 9 7 9 9 9 9 2 2]\n",
      "  sample 3:\n",
      "    input     > [6 7 9 6 8 8 4 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 8 8 8 8 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4700\n",
      "  minibatch loss: 0.941897630692\n",
      "  sample 1:\n",
      "    input     > [4 8 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 5 9 4 9 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 9 9 9 9 9 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 3 7 6 9 4 7 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 7 9 7 7 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4800\n",
      "  minibatch loss: 0.96488404274\n",
      "  sample 1:\n",
      "    input     > [5 3 9 4 5 5 5 4 7 9 5 4 7 6 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 4 4 4 4 7 4 7 7 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 3 9 3 3 4 3 5 6 4 3 8 9 5 4 2 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 4 3 4 3 3 3 3 3 3 3 3 3 2 2 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 4900\n",
      "  minibatch loss: 0.76990789175\n",
      "  sample 1:\n",
      "    input     > [7 4 6 7 3 7 9 6 4 8 4 9 5 4 4 6 6 4 9 3 2 0 0 0 0 0]\n",
      "    predicted > [7 7 4 7 4 6 6 6 6 6 6 6 4 4 4 4 4 5 5 2 2 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 3 9 6 8 6 4 6 7 8 4 6 9 7 4 4 4 6 7 7 2 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 2 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 6 6 4 4 3 9 5 4 8 6 5 9 3 7 2 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 4 6 6 6 6 6 6 4 4 4 4 6 2 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 5000\n",
      "  minibatch loss: 0.962864816189\n",
      "  sample 1:\n",
      "    input     > [3 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 9 8 7 7 8 9 9 4 5 3 4 4 2 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 4 4 4 4 9 4 9 9 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5100\n",
      "  minibatch loss: 0.957665324211\n",
      "  sample 1:\n",
      "    input     > [8 5 8 4 6 3 3 3 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 3 3 3 3 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 6 6 9 4 6 6 9 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 6 6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 9 3 5 7 4 7 9 7 3 7 7 9 6 4 5 5 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 2 2 0 0 0 0 0 0 0 0]\n",
      "batch 5200\n",
      "  minibatch loss: 0.975641310215\n",
      "  sample 1:\n",
      "    input     > [7 9 4 9 5 9 3 6 7 4 8 4 5 3 8 4 4 7 8 9 3 5 7 2 0 0]\n",
      "    predicted > [7 9 9 9 9 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 7 4 7 2 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 7 4 9 6 9 9 8 5 7 5 9 6 9 7 6 9 9 8 5 6 9 2 0 0 0]\n",
      "    predicted > [8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 3 9 8 6 4 5 7 3 3 8 8 4 4 6 9 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 9 4 4 4 4 4 4 8 8 8 8 8 2 0 0 0 0 0 0 0 0]\n",
      "batch 5300\n",
      "  minibatch loss: 0.944722831249\n",
      "  sample 1:\n",
      "    input     > [9 7 3 3 3 4 6 4 8 5 9 9 4 5 4 7 6 4 5 2 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 3 4 4 5 4 5 4 4 5 4 5 5 5 5 5 5 2 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 5 8 9 9 6 3 4 4 8 3 4 8 6 3 3 7 5 7 3 6 6 3 8 2 0]\n",
      "    predicted > [8 8 8 3 3 3 3 3 3 3 3 6 6 6 6 6 6 3 3 3 3 3 3 3 2 0]\n",
      "batch 5400\n",
      "  minibatch loss: 0.965154409409\n",
      "  sample 1:\n",
      "    input     > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 9 9 5 7 7 4 6 5 6 8 4 4 9 4 4 5 7 5 6 5 6 9 3 5 2]\n",
      "    predicted > [9 9 9 9 9 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2]\n",
      "  sample 3:\n",
      "    input     > [7 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 5500\n",
      "  minibatch loss: 0.749831020832\n",
      "  sample 1:\n",
      "    input     > [8 3 3 5 7 6 5 3 9 9 6 9 4 8 3 3 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 3 7 7 9 5 9 5 9 9 5 8 5 8 7 4 3 2 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 5 5 5 5 5 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0]\n",
      "batch 5600\n",
      "  minibatch loss: 0.907332777977\n",
      "  sample 1:\n",
      "    input     > [5 4 9 9 5 3 6 4 5 4 3 3 5 6 9 5 6 2 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 6 5 6 5 3 3 3 3 5 3 5 3 2 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 3 9 4 4 4 4 9 6 9 5 3 5 4 6 9 7 5 8 9 4 5 3 5 7 2]\n",
      "    predicted > [9 9 4 4 4 4 4 5 4 4 5 4 9 4 9 5 9 9 9 9 9 9 9 9 9 2]\n",
      "  sample 3:\n",
      "    input     > [6 6 5 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 5700\n",
      "  minibatch loss: 0.966015279293\n",
      "  sample 1:\n",
      "    input     > [7 4 7 6 8 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 6 6 6 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 9 9 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 9 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 3 3 4 7 9 3 8 5 3 7 9 6 7 5 5 9 4 5 4 4 2 0 0 0 0]\n",
      "    predicted > [6 3 7 7 4 7 5 5 5 5 5 5 5 5 5 5 5 5 5 5 2 2 0 0 0 0]\n",
      "batch 5800\n",
      "  minibatch loss: 0.763842821121\n",
      "  sample 1:\n",
      "    input     > [5 7 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 3 9 3 8 4 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 3 3 3 9 3 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 6 3 7 6 8 5 6 8 5 9 8 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 5900\n",
      "  minibatch loss: 0.915278971195\n",
      "  sample 1:\n",
      "    input     > [5 4 6 9 9 8 3 5 5 5 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 5 5 5 5 5 5 5 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 4 4 5 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 4 4 4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 6000\n",
      "  minibatch loss: 0.763717770576\n",
      "  sample 1:\n",
      "    input     > [5 6 3 7 8 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 3 3 3 3 3 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 5 9 8 9 4 3 8 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 9 9 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 6 5 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 5 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "batch 6100\n",
      "  minibatch loss: 1.03656876087\n",
      "  sample 1:\n",
      "    input     > [9 7 5 8 9 6 6 8 9 3 5 9 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 5 3 7 7 5 7 7 3 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 7 3 3 3 3 3 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 9 9 8 9 5 3 3 6 5 9 8 5 6 4 7 7 8 2 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 5 9 5 5 4 4 4 8 8 7 7 7 7 7 7 2 0 0 0 0 0 0]\n",
      "Training Interrupted\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 500000\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "try:\n",
    "    batch_ix = 0\n",
    "    loss_track = []\n",
    "    batches_gen = batchify_data(toy_data_generator(VOCAB_SIZE, DATA_SIZE, MAX_SEQ_LENGTH), BATCH_SIZE)\n",
    "    \n",
    "    for data_batch in batches_gen: \n",
    "        enc_inc, enc_lengths = pad_data(data_batch, append_suf=[2])\n",
    "        enc_len = lengths\n",
    "        dec_tar, dec_lengths = pad_data(data_batch, append_suf=[2])\n",
    "        \n",
    "        fd = {\n",
    "            enc_inputs: enc_inc.T,\n",
    "            enc_inputs_len: enc_lengths,\n",
    "            dec_targets: dec_tar.T,\n",
    "            dec_targets_len: dec_lengths\n",
    "        }\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch_ix == 0 or batch_ix % 100 == 0:\n",
    "            print('batch {}'.format(batch_ix))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(dec_preds, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[enc_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "        batch_ix += 1\n",
    "                    \n",
    "except KeyboardInterrupt:\n",
    "    print 'Training Interrupted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
