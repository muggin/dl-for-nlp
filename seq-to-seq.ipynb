{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import codecs\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "\n",
    "from src.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_A_PATH, CORPUS_B_PATH = './corpora/europarl.de-en.en', './corpora/europarl.de-en.de'\n",
    "# CORPUS_A_PATH, CORPUS_B_PATH = './corpora/europarl.pl-en.pls', './corpora/europarl.pl-en.ens'\n",
    "\n",
    "# LOAD CORPUS\n",
    "corpus_a, vocab_cnt_a = load_corpus(CORPUS_A_PATH)\n",
    "corpus_b, vocab_cnt_b = load_corpus(CORPUS_B_PATH)\n",
    "\n",
    "raw_corpus_a_size = sum(vocab_cnt_a.itervalues())\n",
    "raw_vocab_a_size = len(vocab_cnt_a)\n",
    "raw_corpus_b_size = sum(vocab_cnt_b.itervalues())\n",
    "raw_vocab_b_size = len(vocab_cnt_b)\n",
    "\n",
    "print 'Corpus A size (total tokens):', raw_corpus_a_size\n",
    "print 'Corpus A vocabulary size (distinct tokens):', raw_vocab_a_size\n",
    "print 'Most popular words (corpus A):', vocab_cnt_a.most_common(5)\n",
    "print\n",
    "print 'Corpus B size (total tokens):', raw_corpus_b_size\n",
    "print 'Corpus B vocabulary size (distinct tokens):', raw_vocab_b_size\n",
    "print 'Most popular words (corpus B):', vocab_cnt_b.most_common(5)\n",
    "\n",
    "# visualize distribution\n",
    "counts_a = sorted(vocab_cnt_a.itervalues(), reverse=True)\n",
    "counts_b = sorted(vocab_cnt_b.itervalues(), reverse=True)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(range(len(counts_a)), counts_a)\n",
    "plt.title('Distribution of token occurences (Corpus SRC)')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurences')\n",
    "plt.grid()\n",
    "plt.subplot(122)\n",
    "plt.semilogy(range(len(counts_b)), counts_b)\n",
    "plt.title('Distribution of token occurences (Corpus TRG)')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurences')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT VOCABS\n",
    "LANG_A_TOKEN_LIMIT = 25000\n",
    "#vocab_a, vocab_enc_a, vocab_dec_a = code_tokens(vocab_cnt_a, LANG_A_TOKEN_LIMIT)\n",
    "corpus_a_enc = [[vocab_enc_a[word] for word in sentence if word in vocab_enc_a] for sentence in corpus_a]\n",
    "\n",
    "LANG_B_TOKEN_LIMIT = 35000\n",
    "# vocab_b, vocab_enc_b, vocab_dec_b = code_tokens(vocab_cnt_b, LANG_B_TOKEN_LIMIT)\n",
    "corpus_b_enc = [[vocab_enc_b[word] for word in sentence if word in vocab_enc_b] for sentence in corpus_b]\n",
    "del corpus_a\n",
    "del corpus_b\n",
    "\n",
    "print 'Clean corpus A size (total sentences):', len(corpus_a_enc)\n",
    "print 'Clean corpus A vocabulary size (distinct tokens):', len(vocab_a)\n",
    "print\n",
    "print 'Clean corpus B size (total sentences):', len(corpus_b_enc)\n",
    "print 'Clean corpus B vocabulary size (distinct tokens):', len(vocab_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_par = zip(corpus_a_enc, corpus_b_enc)\n",
    "length_diff = Counter([abs(len(a) - len(b)) for a, b in corpus_par])\n",
    "\n",
    "print 'Max length diff:', max(length_diff.keys())\n",
    "print 'Avg length diff:', sum(length_diff.keys()) / len(corpus_par)\n",
    "\n",
    "keys, values = zip(*sorted(length_diff.iteritems(), key=lambda x: x[0]))\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogy(keys, values)\n",
    "plt.title('Distribution of length differences (Corpus Parallel)')\n",
    "plt.xlabel('Length Difference')\n",
    "plt.ylabel('Occurences')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqToSeq(object):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, emb_size=100, enc_units=100, dec_units=100, \n",
    "                 num_layers=1, bi_dir=False, learning_rate=1e-3, pad_token=0, eos_token=2):\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.enc_units = enc_units\n",
    "        self.dec_units = dec_units\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_dir = bi_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.pad_token = pad_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _init_placeholders(self):\n",
    "        with tf.variable_scope('placeholders') as scope:\n",
    "            self.enc_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "            self.dec_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "            self.dec_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "            self.enc_inputs_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_len')\n",
    "            self.dec_inputs_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_len')\n",
    "            self.max_dec_inputs_len = tf.reduce_max(self.dec_inputs_len, name='max_decoder_inputs_len')\n",
    "            \n",
    "            self.avg_eval_loss = tf.placeholder_with_default(0.0, shape=None, name='avg_eval_loss')\n",
    "            \n",
    "    def _init_variables(self):\n",
    "        # define global variables\n",
    "        self.global_step = tf.Variable(\n",
    "            initial_value=0, \n",
    "            trainable=False, \n",
    "            name='global_step')\n",
    "        \n",
    "        # define embeddings and lookup\n",
    "        with tf.variable_scope('embeddings') as scope:\n",
    "            self.embeddings_src = tf.Variable(\n",
    "                tf.random_uniform([self.src_vocab_size, self.emb_size], -0.25, 0.25), \n",
    "                dtype=tf.float32,\n",
    "                name='embeddings_src')\n",
    "            self.embeddings_trg = tf.Variable(\n",
    "                tf.random_uniform([self.trg_vocab_size, self.emb_size], -0.25, 0.25), \n",
    "                dtype=tf.float32,\n",
    "                name='embeddings_trg')\n",
    "    \n",
    "    def _init_encoder(self):\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            enc_inputs_emb = tf.nn.embedding_lookup(self.embeddings_src, self.enc_inputs)\n",
    "            enc_cell = tf.contrib.rnn.GRUCell(self.enc_units)\n",
    "            _, self.enc_final_state = tf.nn.dynamic_rnn(\n",
    "                cell=enc_cell, \n",
    "                inputs=enc_inputs_emb, \n",
    "                sequence_length=self.enc_inputs_len, \n",
    "                time_major=False, \n",
    "                dtype=tf.float32, \n",
    "                scope='encoder_cell')\n",
    "        \n",
    "    def _init_decoder(self):\n",
    "        # training decoder\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            dec_inputs_emb = tf.nn.embedding_lookup(self.embeddings_trg, self.dec_inputs)\n",
    "            dec_out_layer = layers_core.Dense(self.trg_vocab_size)\n",
    "            dec_cell = tf.contrib.rnn.GRUCell(self.dec_units)\n",
    "\n",
    "            dec_train_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs=dec_inputs_emb,\n",
    "                sequence_length=self.dec_inputs_len,\n",
    "                time_major=False,\n",
    "                name='training_helper')\n",
    "\n",
    "            dec_train_decoder = seq2seq.BasicDecoder(\n",
    "                cell=dec_cell,\n",
    "                helper=dec_train_helper,\n",
    "                initial_state=self.enc_final_state,\n",
    "                output_layer=dec_out_layer)\n",
    "\n",
    "            self.dec_train_outputs = seq2seq.dynamic_decode(\n",
    "                decoder=dec_train_decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=self.max_dec_inputs_len)[0]\n",
    "            \n",
    "            # inference decoder\n",
    "            batch_size = tf.shape(self.enc_inputs)[0]\n",
    "            eos_slice = tf.fill([batch_size], self.eos_token, name='EOS')\n",
    "\n",
    "            dec_infer_helper = seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding=self.embeddings_trg,\n",
    "                start_tokens=eos_slice,\n",
    "                end_token=self.eos_token)\n",
    "\n",
    "            dec_infer_decoder = seq2seq.BasicDecoder(\n",
    "                cell=dec_cell,\n",
    "                helper=dec_infer_helper,\n",
    "                initial_state=self.enc_final_state,\n",
    "                output_layer=dec_out_layer)\n",
    "\n",
    "            self.dec_infer_outputs = seq2seq.dynamic_decode(\n",
    "                decoder=dec_infer_decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True)[0]\n",
    "    \n",
    "    def _init_optimizer(self):\n",
    "        with tf.variable_scope('optimization') as scope:\n",
    "            dec_train_logits = tf.identity(self.dec_train_outputs.rnn_output)\n",
    "            dec_infer_logits = tf.identity(self.dec_infer_outputs.rnn_output)\n",
    "            self.dec_train_preds = self.dec_train_outputs.sample_id \n",
    "            self.dec_infer_preds = self.dec_infer_outputs.sample_id\n",
    "\n",
    "            masks = tf.sequence_mask(\n",
    "                lengths=self.dec_inputs_len, \n",
    "                maxlen=self.max_dec_inputs_len,\n",
    "                dtype=tf.float32, \n",
    "                name='masks')\n",
    "\n",
    "            self.loss = seq2seq.sequence_loss(\n",
    "                logits=dec_train_logits,\n",
    "                targets=self.dec_targets,\n",
    "                weights=masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True)\n",
    "\n",
    "            # setup optimizer and training step\n",
    "            self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            \n",
    "            trainable_params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, trainable_params)\n",
    "            # add gradient clipping\n",
    "            clip_gradients = gradients\n",
    "            self.updates = self.opt.apply_gradients(\n",
    "                zip(clip_gradients, trainable_params), \n",
    "                global_step=self.global_step)\n",
    "    \n",
    "            # summaries\n",
    "            self.train_summary = tf.summary.scalar('train_loss', self.loss)\n",
    "            self.valid_summary = tf.summary.scalar('valid_loss', self.loss)\n",
    "            self.avg_valid_summary = tf.summary.scalar('avg_valid_loss', self.avg_eval_loss)\n",
    "            \n",
    "    def _build_model(self):\n",
    "        self._init_placeholders()\n",
    "        self._init_variables()\n",
    "        self._init_encoder()\n",
    "        self._init_decoder()\n",
    "        self._init_optimizer()\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def _get_feed_dict(self, enc_in, enc_in_len, dec_in=None, dec_in_len=None, dec_out=None):\n",
    "            feed_dict = { self.enc_inputs: enc_in, self.enc_inputs_len: enc_in_len}\n",
    "            \n",
    "            if dec_in is not None:\n",
    "                feed_dict[self.dec_inputs] = dec_in\n",
    "            if dec_in_len is not None:\n",
    "                feed_dict[self.dec_inputs_len] = dec_in_len\n",
    "            if dec_out is not None:\n",
    "                feed_dict[self.dec_targets] = dec_out\n",
    "                \n",
    "            return feed_dict\n",
    "    \n",
    "    def train(self, sess, enc_inputs, enc_inputs_len, dec_inputs, dec_inputs_len, dec_targets):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len, \n",
    "                                 dec_inputs, dec_inputs_len, \n",
    "                                 dec_targets)\n",
    "        \n",
    "        operations = [self.updates, self.loss, self.train_summary, self.enc_final_state]\n",
    "        _, l, s, e = sess.run(operations, fd)\n",
    "        return l, s, e\n",
    "    \n",
    "    def evaluate(self, sess, enc_inputs, enc_inputs_len, dec_inputs, dec_inputs_len, dec_targets):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len, \n",
    "                                 dec_inputs, dec_inputs_len, \n",
    "                                 dec_targets)\n",
    "        \n",
    "        operations = [self.loss, self.valid_summary, self.dec_train_preds, self.enc_final_state]\n",
    "        l, s, p, e = sess.run(operations, fd)\n",
    "        return l, s, p, e\n",
    "    \n",
    "    def infer(self, sess, enc_inputs, enc_inputs_len):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len)\n",
    "        return sess.run([self.dec_infer_preds], fd)\n",
    "    \n",
    "    def encode_seq(self, sess, enc_inputs, enc_inputs_len):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len)\n",
    "        return sess.run([self.enc_final_state], fd)\n",
    "    \n",
    "    def save_model(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path=path, global_step=self.global_step)\n",
    "    \n",
    "    def restore_model(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example - Reconstructing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT PARAMETERS\n",
    "VOCAB_SIZE_SRC = VOCAB_SIZE_TRG = 10\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_DATA_SIZE = 500000\n",
    "VAL_DATA_SIZE = 500\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "EMB_SIZE = 20\n",
    "ENC_UNITS = DEC_UNITS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HARD_MAX_LEN = 128\n",
    "\n",
    "SUMM_INTERVAL = 100\n",
    "EVAL_INTERVAL = 250\n",
    "CKPT_INTERVAL = 1000\n",
    "\n",
    "CKPT_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "EXPERIMENT_NAME = 'toy-example'\n",
    "\n",
    "# EXPERIMENT DATA\n",
    "train_data = toy_data_generator(VOCAB_SIZE, TRAIN_DATA_SIZE, 15, 3)\n",
    "eval_data = list(toy_data_generator(VOCAB_SIZE, VAL_DATA_SIZE, 15, 3))\n",
    "vocab_dec_a = {ix: str(ix) for ix in xrange(11)}\n",
    "vocab_dec_b = vocab_dec_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Europarl DE-EN Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT PARAMETERS\n",
    "VOCAB_SIZE_SRC = len(vocab_a)\n",
    "VOCAB_SIZE_TRG = len(vocab_b)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "EMB_SIZE = 200\n",
    "ENC_UNITS = 500\n",
    "DEC_UNITS = ENC_UNITS\n",
    "LEARNING_RATE = 0.001\n",
    "HARD_MAX_LEN = 128\n",
    "\n",
    "SUMM_INTERVAL = 100\n",
    "EVAL_INTERVAL = 250\n",
    "CKPT_INTERVAL = 1000\n",
    "\n",
    "CKPT_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "EXPERIMENT_NAME = 'ep-de'\n",
    "\n",
    "# EXPERIMENT DATA\n",
    "random.seed(1)\n",
    "random.shuffle(corpus_par)\n",
    "\n",
    "split data\n",
    "corpus_len = len(corpus_par)\n",
    "train_split, eval_split, test_split = int(0.8*corpus_len), int(0.1*corpus_len), int(0.1*corpus_len)\n",
    "train_data = corpus_par[:train_split]\n",
    "eval_data = corpus_par[train_split:train_split+eval_split]\n",
    "test_data = corpus_par[-test_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = SeqToSeq(VOCAB_SIZE_SRC, VOCAB_SIZE_TRG, EMB_SIZE, ENC_UNITS, DEC_UNITS, learning_rate=LEARNING_RATE)\n",
    "\n",
    "try:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join(LOG_PATH, EXPERIMENT_NAME + time.strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "        graph=sess.graph)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batches_gen = batchify_data(train_data, BATCH_SIZE)\n",
    "        train_losses = []\n",
    "        for data_batch in batches_gen: \n",
    "            batch_src, batch_trg = zip(*data_batch)\n",
    "            \n",
    "            # prepare batch\n",
    "            enc_inp, enc_lengths = pad_data(batch_src, append_suf=[2])\n",
    "            dec_inp, _ = pad_data(batch_trg, append_pre=[2])\n",
    "            dec_trg, dec_lengths = pad_data(batch_trg, append_suf=[2])\n",
    "            \n",
    "            # training step\n",
    "            if enc_inp.shape[1] > HARD_MAX_LEN: continue\n",
    "            l, s, _ = model.train(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "            train_losses.append(l)\n",
    "            \n",
    "            # summarize, eval, etc.\n",
    "            global_step = model.global_step.eval()\n",
    "            \n",
    "            if global_step % CKPT_INTERVAL == 0:\n",
    "                ckpt_file = os.path.join(CKPT_PATH, EXPERIMENT_NAME + time.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "                model.save_model(sess, ckpt_file)\n",
    "                print 'Saved model...'\n",
    "                \n",
    "            if global_step == 1 or global_step % SUMM_INTERVAL == 0:\n",
    "                summary_writer.add_summary(s, global_step)\n",
    "\n",
    "            if global_step == 1 or global_step % EVAL_INTERVAL == 0:\n",
    "                eval_losses = []\n",
    "                for batch_data in batchify_data(eval_data, BATCH_SIZE): \n",
    "                    batch_src, batch_trg = zip(*batch_data)\n",
    "                    enc_inp, enc_lengths = pad_data(batch_src, append_suf=[2])\n",
    "                    dec_inp, _ = pad_data(batch_trg, append_pre=[2])\n",
    "                    dec_trg, dec_lengths = pad_data(batch_trg, append_suf=[2])\n",
    "                    l, _, p, _ = model.evaluate(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "                    eval_losses.append(l)\n",
    "                    \n",
    "                eval_s = sess.run(model.avg_valid_summary, {model.avg_eval_loss: np.mean(eval_losses)})\n",
    "                summary_writer.add_summary(eval_s, global_step)\n",
    "                \n",
    "                print('batch {}'.format(global_step))\n",
    "                print('train losses: {} / eval losses: {}'.format(np.mean(train_losses), np.mean(eval_losses)))\n",
    "                for i, (inp, pred) in enumerate(zip(enc_inp, p)[:3]):\n",
    "                    print('sample {}:'.format(i + 1))\n",
    "                    print('input     >> {}'.format(' '.join([vocab_dec_a[word].encode('ascii', errors='replace') for word in inp])))\n",
    "                    print('predicted >> {}'.format(' '.join([vocab_dec_b[word].encode('ascii', errors='replace') for word in pred])))\n",
    "                    \n",
    "                # clear train losses\n",
    "                train_losses = []\n",
    "        summary_writer.flush()\n",
    "                    \n",
    "except KeyboardInterrupt:\n",
    "    ckpt_file = os.path.join(CKPT_PATH, EXPERIMENT_NAME + time.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    model.save_model(sess, ckpt_file)\n",
    "    summary_writer.close()\n",
    "    print 'Training Interrupted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = SeqToSeq(VOCAB_SIZE_SRC, VOCAB_SIZE_TRG, EMB_SIZE, ENC_UNITS, DEC_UNITS, learning_rate=LEARNING_RATE)\n",
    "#model.restore_model(sess, './models/basic/ep-de2017-10-09-17-21-06-11000')\n",
    "model.restore_model(sess, './models/reversed/ep-de2017-10-10-00-01-03-11000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses, translations = [], []\n",
    "test_data = corpus_par_small\n",
    "data_batches = list(batchify_data(test_data, BATCH_SIZE))\n",
    "for data_batch in tqdm(data_batches): \n",
    "    batch_src, batch_trg = zip(*data_batch)\n",
    "    enc_inp = [line[::-1] for line in enc_inp]\n",
    "    enc_inp, enc_lengths = pad_data(batch_src, append_suf=[2])\n",
    "    dec_inp, _ = pad_data(batch_trg, append_pre=[2])\n",
    "    dec_trg, dec_lengths = pad_data(batch_trg, append_suf=[2])\n",
    "    if enc_inp.shape[1] > HARD_MAX_LEN: continue\n",
    "    l, _, p, _ = model.evaluate(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "    translations.append(model.infer(sess, enc_inp, enc_lengths))\n",
    "    test_losses.append(l)\n",
    "    \n",
    "# flatten and cleanup network output\n",
    "trans = [trans.tolist() for wrap_batch in translations for batch in wrap_batch for trans in batch]\n",
    "trans_end_ix = [line.index(2) for line in trans]\n",
    "trans_pred = [line[:end_ix] for line, end_ix in zip(trans, trans_end_ix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute BLEU score and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words = [[vocab_dec_b[token] for token in line] for line in trans_pred]\n",
    "targ_words = [[vocab_dec_b[token] for token in line[1]] for line in corpus_par_small]\n",
    "bleu.corpus_bleu([[sentence] for sentence in targ_words], pred_words, weights=(0.4,0.3,0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in xrange(len(test_data)):\n",
    "    print 'SRC: ', ' '.join([vocab_dec_a[token] for token in test_data[ix][0]])\n",
    "    print 'TARGET: ', ' '.join([vocab_dec_b[token] for token in test_data[ix][1]])\n",
    "    print 'PEDICT: ', ' '.join([vocab_dec_b[token] for token in trans_pred[ix]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save translations to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('test-source.en', 'w', encoding='utf-8') as test_src:\n",
    "    with codecs.open('test-target.de', 'w', encoding='utf-8') as test_trg:\n",
    "        with codecs.open('test-pred.de', 'w', encoding='utf-8') as test_prd:\n",
    "            for ix in xrange(len(test_data)):\n",
    "                test_src.write(' '.join([vocab_dec_a[token] for token in test_data[ix][0]]) + '\\n')\n",
    "                test_trg.write(' '.join([vocab_dec_b[token] for token in test_data[ix][1]]) + '\\n')\n",
    "                test_prd.write(' '.join([vocab_dec_b[token] for token in trans_pred[ix]]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = ['italy', 'poland', 'england', 'germany',\n",
    "                 'monday', 'tuesday', 'thursday', 'friday',\n",
    "                 'summer', 'winter', 'autumn', 'spring']\n",
    "src_sentences = ['mr president we must solve this problem']\n",
    "\n",
    "src_split = [sentence.split(' ') for sentence in src_sentences]\n",
    "src_enc = [[vocab_enc_a[word] for word in sent] for sent in src_split]\n",
    "enc_inp, enc_len = pad_data(src_enc, append_suf=[2])\n",
    "print 'INPUT: ', ' '.join([vocab_dec_a[token] for token in src_enc[0]])\n",
    "print 'OUTPUT:', ' '.join([vocab_dec_b[token].encode('utf8') for token in model.infer(sess, enc_inp, enc_len)[0][0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = [[3,4,6,8,9,2]]\n",
    "enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "print 'I >>', batch_data\n",
    "print 'O >>', model.infer(sess, enc_inp, enc_lengths)[0]\n",
    "\n",
    "batch_data = [[3,4,6,8,9], [3,4,6,8,8], [3,4,6,8,3], [5,5,5,6,7], [5,5,5,5,6,9], [2,2,2,2,4]]\n",
    "enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "data_enc = model.encode_seq(sess, enc_inp, enc_lengths)[0]\n",
    "data_enc = data_enc / np.linalg.norm(data_enc, axis=1, keepdims=True)\n",
    "print 'Similarites:', data_enc.dot(data_enc[0].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
