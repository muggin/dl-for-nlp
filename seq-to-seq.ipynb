{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import codecs\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.contrib.layers import safe_embedding_lookup_sparse\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    \"\"\" Load corpus from text file and tokenize \"\"\"\n",
    "    corpus = []\n",
    "    vocab_cnt = Counter()\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    \n",
    "    with codecs.open(file_path, 'r', encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            # clean lines from any punctuation characters\n",
    "            clean_line = re.sub('[\\+\\-\\.\\,\\:\\;\\\"\\?\\!\\>\\<\\=\\(\\)\\n]+', '', line)\n",
    "            tokens = tokenizer.tokenize(clean_line.lower())\n",
    "            corpus.append(tokens)\n",
    "            vocab_cnt.update(tokens)\n",
    "            \n",
    "    return corpus, vocab_cnt\n",
    "\n",
    "\n",
    "def code_tokens(vocab_cnt, max_size=30000, unk_symbol='<unk>'):\n",
    "    \"\"\" Filter vocabulary and encode tokens \"\"\"\n",
    "    vocab = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "    vocab.extend([word for word, _ in vocab_cnt.most_common(max_size)])\n",
    "    vocab_enc = {token: ix for ix, token in enumerate(vocab)}\n",
    "    vocab_dec = {ix: token for token, ix in vocab_enc.iteritems()}\n",
    "    \n",
    "    return vocab, vocab_enc, vocab_dec\n",
    "\n",
    "\n",
    "def generate_context_data(corpus, max_window_size=5, skip_size=1, flatten=True):\n",
    "    \"\"\" Generate data with context in format (target, [contexts]) or (target, context) \"\"\"\n",
    "    for center_ix in xrange(max_window_size, len(corpus)-max_window_size, skip_size):\n",
    "        # sample a window size for the given center word\n",
    "        window_size = np.random.randint(max_window_size) + 1\n",
    "        full_context = corpus[center_ix-window_size:center_ix] + corpus[center_ix+1: center_ix+window_size+1]\n",
    "        \n",
    "        if flatten:\n",
    "            for context_ix in xrange(2*window_size):\n",
    "                yield (corpus[center_ix], full_context[context_ix])\n",
    "        else:\n",
    "            yield(corpus[center_ix], full_context)\n",
    "\n",
    "\n",
    "def pad_data(data_arr, append_pre=[], append_suf=[], max_length=None):\n",
    "    data_arr = [append_pre + row + append_suf for row in data_arr]\n",
    "    lengths = [len(row) for row in data_arr]\n",
    "    max_len = max(lengths) if not max_length else max_length\n",
    "    return np.array([row+[0]*(max_len-length) for row, length in zip(data_arr, lengths)]), lengths\n",
    "    \n",
    "                \n",
    "def batchify_data(data_generator, batch_size):\n",
    "    \"\"\" Split dataset (generator) into batches \"\"\"\n",
    "    if isinstance(data_generator, list):\n",
    "        for ix in xrange(0, len(data_generator), batch_size):\n",
    "            buff = data_generator[ix:ix+batch_size]\n",
    "            yield buff\n",
    "    else:\n",
    "        while data_generator:\n",
    "            buff = []\n",
    "            for ix in xrange(0, batch_size):\n",
    "                buff.append(next(data_generator))\n",
    "            yield buff\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings_obj, file_name):\n",
    "    \"\"\" Save word embeddings and helper structures \"\"\"\n",
    "    with open(file_name, 'wb') as fd:\n",
    "        pickle.dump(embeddings_obj, fd)\n",
    "    \n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    \"\"\" Load word embeddings and helper structures \"\"\"\n",
    "    with open(file_name, 'r') as fd:\n",
    "        embeddings_obj = pickle.load(fd)\n",
    "    return embeddings_obj\n",
    "    \n",
    "    \n",
    "def get_tsne_embeddings(embedding_matrix):\n",
    "    \"\"\" Compute t-SNE representation of embeddings \"\"\"\n",
    "    tsne = TSNE(perplexity=25, n_components=2, init='pca', n_iter=5000)\n",
    "    return tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "\n",
    "def get_pca_embeddings(embedding_matrix):\n",
    "    \"\"\" Compute PCA representation of embeddings \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    return pca.fit_transform(embedding_matrix)\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings, words=[], words_cnt=500, method='pca', figsize=(8,8)):\n",
    "    \"\"\" Plot subset of embeddings in 2D space using t-SNE or PCA \"\"\"\n",
    "    embedding_matrix = embeddings._embeddings\n",
    "    vocab_dec = embeddings._vocab_dec\n",
    "    vocab_enc = embeddings._vocab_enc\n",
    "    \n",
    "    # prepare data\n",
    "    if not words:\n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        ixs = range(vocab_size)\n",
    "        random.shuffle(ixs)\n",
    "        chosen_ixs = ixs[:words_cnt]\n",
    "        labels = [vocab_dec[ix] for ix in chosen_ixs]\n",
    "        word_vecs = embedding_matrix[chosen_ixs]\n",
    "    else:\n",
    "        labels = words\n",
    "        chosen_ixs = [vocab_enc[word] for word in words]\n",
    "        word_vecs = embedding_matrix[chosen_ixs]\n",
    "        \n",
    "    if method == 'tsne':\n",
    "        low_dim_embeddings = get_tsne_embeddings(word_vecs)\n",
    "    else:\n",
    "        low_dim_embeddings = get_pca_embeddings(word_vecs)\n",
    "        \n",
    "    # plot reduced vectors\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for embedding, label in zip(low_dim_embeddings, labels):\n",
    "        x, y = embedding[0], embedding[1]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', \n",
    "                     va='bottom')\n",
    "    plt.yticks=[]\n",
    "    plt.xticks=[]\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "class Embeddings(object):\n",
    "    \"\"\" Class wrapping word embeddings \"\"\"\n",
    "    def __init__(self, embedding_matrix, vocab_enc, vocab_dec):\n",
    "        self._embeddings = embedding_matrix\n",
    "        self._vocab_enc = vocab_enc\n",
    "        self._vocab_dec = vocab_dec\n",
    "    \n",
    "    def find_embedding(self, word):\n",
    "        \"\"\" Find embedding for a given word \"\"\"\n",
    "        if isinstance(word, str):\n",
    "            word = self._vocab_enc[word]\n",
    "        return self._embeddings[word]\n",
    "    \n",
    "    def find_neighbors(self, word, k=5, nearest=True, exclude=[], include_scores=False):\n",
    "        \"\"\" Find neighboring words (semantic regularities) \"\"\"\n",
    "        word_ix = self._vocab_enc[word]\n",
    "        exclude = exclude + [word_ix]\n",
    "        \n",
    "        # find neighbors\n",
    "        word_emb = self._embeddings[word_ix]\n",
    "        similarities = self._embeddings.dot(word_emb)\n",
    "        similarities[exclude] = 0\n",
    "        best_matches = np.argsort(similarities)\n",
    "        trimmed_matches = best_matches[-k:][::-1] if nearest else best_matches[:k]\n",
    "        return [(self._vocab_dec[word_ix], similarities[word_ix]) for word_ix in trimmed_matches]\n",
    "    \n",
    "    def find_analogous(self, word_a, word_b, word_c, k=5):\n",
    "        \"\"\" Find analogous word (syntactic regularities: word_a - word_b = x - word_c) \"\"\"\n",
    "        word_a_ix, word_b_ix, word_c_ix = [self._vocab_enc[word] for word in [word_a, word_b, word_c]]\n",
    "        exclude = [word_a_ix, word_b_ix, word_c_ix]\n",
    "        \n",
    "        emb_a = self.find_embedding(word_a_ix) \n",
    "        emb_b = self.find_embedding(word_b_ix) \n",
    "        emb_c = self.find_embedding(word_c_ix) \n",
    "        emb_d_hat = emb_a - emb_b + emb_c\n",
    "        similarities = self._embeddings.dot(emb_d_hat)\n",
    "        similarities[exclude] = 0\n",
    "        best_matches = np.argsort(similarities)\n",
    "        trimmed_matches = best_matches[-k:][::-1]\n",
    "        return [(self._vocab_dec[word_ix], similarities[word_ix]) for word_ix in trimmed_matches]\n",
    "    \n",
    "    def vocab(self):\n",
    "        \"\"\" Return vocabulary list \"\"\"\n",
    "        return self._vocab_enc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORPUS_A_PATH, CORPUS_B_PATH = './corpora/europarl.de-en.de', './corpora/europarl.de-en.en'\n",
    "CORPUS_A_PATH, CORPUS_B_PATH = './corpora/europarl.pl-en.pls', './corpora/europarl.pl-en.ens'\n",
    "\n",
    "# LOAD CORPUS\n",
    "corpus_a, vocab_cnt_a = load_corpus(CORPUS_A_PATH)\n",
    "corpus_b, vocab_cnt_b = load_corpus(CORPUS_B_PATH)\n",
    "\n",
    "raw_corpus_a_size = sum(vocab_cnt_a.itervalues())\n",
    "raw_vocab_a_size = len(vocab_cnt_a)\n",
    "raw_corpus_b_size = sum(vocab_cnt_b.itervalues())\n",
    "raw_vocab_b_size = len(vocab_cnt_b)\n",
    "\n",
    "print 'Corpus A size (total tokens):', raw_corpus_a_size\n",
    "print 'Corpus A vocabulary size (distinct tokens):', raw_vocab_a_size\n",
    "print 'Most popular words (corpus A):', vocab_cnt_a.most_common(5)\n",
    "print\n",
    "print 'Corpus B size (total tokens):', raw_corpus_b_size\n",
    "print 'Corpus B vocabulary size (distinct tokens):', raw_vocab_b_size\n",
    "print 'Most popular words (corpus B):', vocab_cnt_b.most_common(5)\n",
    "\n",
    "# visualize distribution\n",
    "counts_a = sorted(vocab_cnt_a.itervalues(), reverse=True)\n",
    "counts_b = sorted(vocab_cnt_b.itervalues(), reverse=True)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(range(len(counts_a)), counts_a)\n",
    "plt.title('Distribution of token occurences (Corpus SRC)')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurences')\n",
    "plt.grid()\n",
    "plt.subplot(122)\n",
    "plt.semilogy(range(len(counts_b)), counts_b)\n",
    "plt.title('Distribution of token occurences (Corpus TRG)')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurences')\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT VOCABS\n",
    "LANG_A_TOKEN_LIMIT = 45000\n",
    "vocab_a, vocab_enc_a, vocab_dec_a = code_tokens(vocab_cnt_a, LANG_A_TOKEN_LIMIT)\n",
    "corpus_a_enc = [[vocab_enc_a[word] for word in sentence if word in vocab_enc_a] for sentence in corpus_a]\n",
    "\n",
    "LANG_B_TOKEN_LIMIT = 25000\n",
    "vocab_b, vocab_enc_b, vocab_dec_b = code_tokens(vocab_cnt_b, LANG_B_TOKEN_LIMIT)\n",
    "corpus_b_enc = [[vocab_enc_b[word] for word in sentence if word in vocab_enc_b] for sentence in corpus_b]\n",
    "\n",
    "print 'Clean corpus A size (total sentences):', len(corpus_a_enc)\n",
    "print 'Clean corpus A vocabulary size (distinct tokens):', len(vocab_a)\n",
    "print\n",
    "print 'Clean corpus B size (total sentences):', len(corpus_b_enc)\n",
    "print 'Clean corpus B vocabulary size (distinct tokens):', len(vocab_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_par = zip(corpus_a_enc, corpus_b_enc)\n",
    "length_diff = Counter([abs(len(a) - len(b)) for a, b in corpus_par])\n",
    "\n",
    "print 'Max length diff:', max(length_diff.keys())\n",
    "print 'Avg length diff:', sum(length_diff.keys()) / len(corpus_par)\n",
    "\n",
    "keys, values = zip(*sorted(length_diff.iteritems(), key=lambda x: x[0]))\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.semilogy(keys, values)\n",
    "plt.title('Distribution of length differences (Corpus Parallel)')\n",
    "plt.xlabel('Length Difference')\n",
    "plt.ylabel('Occurences')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeq(object):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, emb_size=100, enc_units=100, dec_units=100, \n",
    "                 num_layers=1, bi_dir=False, learning_rate=1e-3, pad_token=0, eos_token=2):\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.enc_units = enc_units\n",
    "        self.dec_units = dec_units\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_dir = bi_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.pad_token = pad_token\n",
    "        self.eos_token = eos_token\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _init_placeholders(self):\n",
    "        with tf.variable_scope('placeholders') as scope:\n",
    "            self.enc_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "            self.dec_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')\n",
    "            self.dec_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "            self.enc_inputs_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_len')\n",
    "            self.dec_inputs_len = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_inputs_len')\n",
    "            self.max_dec_inputs_len = tf.reduce_max(self.dec_inputs_len, name='max_decoder_inputs_len')\n",
    "            \n",
    "            self.avg_eval_loss = tf.placeholder_with_default(0.0, shape=None, name='avg_eval_loss')\n",
    "            \n",
    "    def _init_variables(self):\n",
    "        # define global variables\n",
    "        self.global_step = tf.Variable(\n",
    "            initial_value=0, \n",
    "            trainable=False, \n",
    "            name='global_step')\n",
    "        \n",
    "        # define embeddings and lookup\n",
    "        with tf.variable_scope('embeddings') as scope:\n",
    "            self.embeddings_src = tf.Variable(\n",
    "                tf.random_uniform([self.src_vocab_size, self.emb_size], -0.25, 0.25), \n",
    "                dtype=tf.float32,\n",
    "                name='embeddings_src')\n",
    "            self.embeddings_trg = tf.Variable(\n",
    "                tf.random_uniform([self.trg_vocab_size, self.emb_size], -0.25, 0.25), \n",
    "                dtype=tf.float32,\n",
    "                name='embeddings_trg')\n",
    "    \n",
    "    def _init_encoder(self):\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            enc_inputs_emb = tf.nn.embedding_lookup(self.embeddings_src, self.enc_inputs)\n",
    "            enc_cell = tf.contrib.rnn.GRUCell(self.enc_units)\n",
    "            _, self.enc_final_state = tf.nn.dynamic_rnn(\n",
    "                cell=enc_cell, \n",
    "                inputs=enc_inputs_emb, \n",
    "                sequence_length=self.enc_inputs_len, \n",
    "                time_major=False, \n",
    "                dtype=tf.float32, \n",
    "                scope='encoder_cell')\n",
    "        \n",
    "    def _init_decoder(self):\n",
    "        # training decoder\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            dec_inputs_emb = tf.nn.embedding_lookup(self.embeddings_trg, self.dec_inputs)\n",
    "            dec_out_layer = layers_core.Dense(self.trg_vocab_size)\n",
    "            dec_cell = tf.contrib.rnn.GRUCell(self.dec_units)\n",
    "\n",
    "            dec_train_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                inputs=dec_inputs_emb,\n",
    "                sequence_length=self.dec_inputs_len,\n",
    "                time_major=False,\n",
    "                name='training_helper')\n",
    "\n",
    "            dec_train_decoder = seq2seq.BasicDecoder(\n",
    "                cell=dec_cell,\n",
    "                helper=dec_train_helper,\n",
    "                initial_state=self.enc_final_state,\n",
    "                output_layer=dec_out_layer)\n",
    "\n",
    "            self.dec_train_outputs = seq2seq.dynamic_decode(\n",
    "                decoder=dec_train_decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=self.max_dec_inputs_len)[0]\n",
    "            \n",
    "            # inference decoder\n",
    "            batch_size = tf.shape(self.enc_inputs)[0]\n",
    "            eos_slice = tf.fill([batch_size], self.eos_token, name='EOS')\n",
    "\n",
    "            dec_infer_helper = seq2seq.GreedyEmbeddingHelper(\n",
    "                embedding=self.embeddings_trg,\n",
    "                start_tokens=eos_slice,\n",
    "                end_token=self.eos_token)\n",
    "\n",
    "            dec_infer_decoder = seq2seq.BasicDecoder(\n",
    "                cell=dec_cell,\n",
    "                helper=dec_infer_helper,\n",
    "                initial_state=self.enc_final_state,\n",
    "                output_layer=dec_out_layer)\n",
    "\n",
    "            self.dec_infer_outputs = seq2seq.dynamic_decode(\n",
    "                decoder=dec_infer_decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True)[0]\n",
    "    \n",
    "    def _init_optimizer(self):\n",
    "        with tf.variable_scope('optimization') as scope:\n",
    "            dec_train_logits = tf.identity(self.dec_train_outputs.rnn_output)\n",
    "            dec_infer_logits = tf.identity(self.dec_infer_outputs.rnn_output)\n",
    "            self.dec_train_preds = self.dec_train_outputs.sample_id \n",
    "            self.dec_infer_preds = self.dec_infer_outputs.sample_id\n",
    "\n",
    "            masks = tf.sequence_mask(\n",
    "                lengths=self.dec_inputs_len, \n",
    "                maxlen=self.max_dec_inputs_len,\n",
    "                dtype=tf.float32, \n",
    "                name='masks')\n",
    "\n",
    "            self.loss = seq2seq.sequence_loss(\n",
    "                logits=dec_train_logits,\n",
    "                targets=self.dec_targets,\n",
    "                weights=masks,\n",
    "                average_across_timesteps=True,\n",
    "                average_across_batch=True)\n",
    "\n",
    "            # setup optimizer and training step\n",
    "            self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            \n",
    "            trainable_params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, trainable_params)\n",
    "            # add gradient clipping\n",
    "            clip_gradients = gradients\n",
    "            self.updates = self.opt.apply_gradients(\n",
    "                zip(clip_gradients, trainable_params), \n",
    "                global_step=self.global_step)\n",
    "    \n",
    "            # summaries\n",
    "            self.train_summary = tf.summary.scalar('train_loss', self.loss)\n",
    "            self.valid_summary = tf.summary.scalar('valid_loss', self.loss)\n",
    "            self.avg_valid_summary = tf.summary.scalar('avg_valid_loss', self.avg_eval_loss)\n",
    "            \n",
    "    def _build_model(self):\n",
    "        self._init_placeholders()\n",
    "        self._init_variables()\n",
    "        self._init_encoder()\n",
    "        self._init_decoder()\n",
    "        self._init_optimizer()\n",
    "        \n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def _get_feed_dict(self, enc_in, enc_in_len, dec_in=None, dec_in_len=None, dec_out=None):\n",
    "            feed_dict = { self.enc_inputs: enc_in, self.enc_inputs_len: enc_in_len}\n",
    "            \n",
    "            if dec_in is not None:\n",
    "                feed_dict[self.dec_inputs] = dec_in\n",
    "            if dec_in_len is not None:\n",
    "                feed_dict[self.dec_inputs_len] = dec_in_len\n",
    "            if dec_out is not None:\n",
    "                feed_dict[self.dec_targets] = dec_out\n",
    "                \n",
    "            return feed_dict\n",
    "    \n",
    "    def train(self, sess, enc_inputs, enc_inputs_len, dec_inputs, dec_inputs_len, dec_targets):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len, \n",
    "                                 dec_inputs, dec_inputs_len, \n",
    "                                 dec_targets)\n",
    "        \n",
    "        operations = [self.updates, self.loss, self.train_summary, self.enc_final_state]\n",
    "        _, l, s, e = sess.run(operations, fd)\n",
    "        return l, s, e\n",
    "    \n",
    "    def evaluate(self, sess, enc_inputs, enc_inputs_len, dec_inputs, dec_inputs_len, dec_targets):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len, \n",
    "                                 dec_inputs, dec_inputs_len, \n",
    "                                 dec_targets)\n",
    "        \n",
    "        operations = [self.loss, self.valid_summary, self.dec_train_preds, self.enc_final_state]\n",
    "        l, s, p, e = sess.run(operations, fd)\n",
    "        return l, s, p, e\n",
    "    \n",
    "    def infer(self, sess, enc_inputs, enc_inputs_len):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len)\n",
    "        return sess.run([self.dec_infer_preds], fd)\n",
    "    \n",
    "    def encode_seq(self, sess, enc_inputs, enc_inputs_len):\n",
    "        fd = self._get_feed_dict(enc_inputs, enc_inputs_len)\n",
    "        return sess.run([self.enc_final_state], fd)\n",
    "    \n",
    "    def save_model(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path=path, global_step=self.global_step)\n",
    "    \n",
    "    def restore_model(self, sess, path):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example - Reconstructing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_generator(vocab_size, data_size, max_seq_length, reserved_digits=3):\n",
    "    for _ in xrange(data_size):\n",
    "        seq_length = np.random.randint(max_seq_length) + 1\n",
    "        yield [np.random.randint(vocab_size-reserved_digits)+reserved_digits for _ in xrange(seq_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_DATA_SIZE = 500000\n",
    "VAL_DATA_SIZE = 500\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "EMB_SIZE = 20\n",
    "ENC_UNITS = 20\n",
    "DEC_UNITS = ENC_UNITS\n",
    "LEARNING_RATE = 0.001\n",
    "HARD_MAX_LEN = 128\n",
    "\n",
    "SUMM_INTERVAL = 100\n",
    "EVAL_INTERVAL = 250\n",
    "CKPT_INTERVAL = 1000\n",
    "\n",
    "CKPT_PATH = './models/'\n",
    "LOG_PATH = './logs/'\n",
    "\n",
    "# setup model\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "train_toy_data = toy_data_generator(VOCAB_SIZE, TRAIN_DATA_SIZE, 15, 3)\n",
    "eval_toy_data = list(toy_data_generator(VOCAB_SIZE, VAL_DATA_SIZE, 15, 3))\n",
    "model = SeqToSeq(VOCAB_SIZE, VOCAB_SIZE, EMB_SIZE, ENC_UNITS, DEC_UNITS, learning_rate=LEARNING_RATE)\n",
    "\n",
    "try:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join(LOG_PATH, 'toy-example-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\")),\n",
    "        graph=sess.graph)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batches_gen = batchify_data(train_toy_data, BATCH_SIZE)\n",
    "        train_losses = []\n",
    "        for batch_data in batches_gen: \n",
    "            # prepare batch\n",
    "            enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "            dec_inp, _ = pad_data(batch_data, append_pre=[2])\n",
    "            dec_trg, dec_lengths = pad_data(batch_data, append_suf=[2])\n",
    "            \n",
    "            # training step\n",
    "            if enc_inp.shape[1] > HARD_MAX_LEN: continue\n",
    "            l, s, _ = model.train(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "            train_losses.append(l)\n",
    "            \n",
    "            # summarize, eval, etc.\n",
    "            global_step = model.global_step.eval()\n",
    "            \n",
    "            if global_step % CKPT_INTERVAL == 0:\n",
    "                ckpt_file = os.path.join(CKPT_PATH, 'toy-example-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "                model.save_model(sess, ckpt_file)\n",
    "                print 'Saved model...'\n",
    "                \n",
    "            if global_step == 1 or global_step % SUMM_INTERVAL == 0:\n",
    "                summary_writer.add_summary(s, global_step)\n",
    "\n",
    "            if global_step == 1 or global_step % EVAL_INTERVAL == 0:\n",
    "                eval_losses = []\n",
    "                for batch_data in batchify_data(eval_toy_data, BATCH_SIZE): \n",
    "                    enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "                    dec_inp, _ = pad_data(batch_data, append_pre=[2])\n",
    "                    dec_trg, dec_lengths = pad_data(batch_data, append_suf=[2])\n",
    "                    l, _, p, _ = model.evaluate(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "                    eval_losses.append(l)\n",
    "                    \n",
    "                eval_s = sess.run(model.avg_valid_summary, {model.avg_eval_loss: np.mean(eval_losses)})\n",
    "                summary_writer.add_summary(eval_s, global_step)\n",
    "                \n",
    "                print('batch {}'.format(global_step))\n",
    "                print('train losses: {} / eval losses: {}'.format(np.mean(train_losses), np.mean(eval_losses)))\n",
    "                for i, (inp, pred) in enumerate(zip(enc_inp, p)[:3]):\n",
    "                    print('sample {}:'.format(i + 1))\n",
    "                    print('input     >> {}'.format(' '.join([str(digit) for digit in inp])))\n",
    "                    print('predicted >> {}'.format(' '.join([str(digit) for digit in pred])))\n",
    "                    \n",
    "                # clear train losses\n",
    "                train_losses = []\n",
    "        summary_writer.flush()\n",
    "                    \n",
    "except KeyboardInterrupt:\n",
    "    model.save_model(sess, ckpt_file)\n",
    "    summary_writer.close()\n",
    "    print 'Training Interrupted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = [[3,4,6,8,9]]\n",
    "enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "print 'I >>', batch_data\n",
    "print 'O >>', model.infer(sess, enc_inp, enc_lengths)[0]\n",
    "\n",
    "batch_data = [[3,4,6,8,9], [3,4,6,8,8], [3,4,6,8,3], [5,5,5,6,7], [5,5,5,5,6,9], [2,2,2,2,4]]\n",
    "enc_inp, enc_lengths = pad_data(batch_data, append_suf=[2])\n",
    "data_enc = model.encode_seq(sess, enc_inp, enc_lengths)[0]\n",
    "data_enc = data_enc / np.linalg.norm(data_enc, axis=1, keepdims=True)\n",
    "print 'Similarites:', data_enc.dot(data_enc[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Europarl DE-EN Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_SRC = len(vocab_a)\n",
    "VOCAB_SIZE_TRG = len(vocab_b)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "EMB_SIZE = 100\n",
    "ENC_HIDDEN_UNITS = 200\n",
    "DEC_HIDDEN_UNITS = ENC_HIDDEN_UNITS\n",
    "LEARNING_RATE = 0.001\n",
    "HARD_MAX_LEN = 128\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "model = SeqToSeq(VOCAB_SIZE_SRC, VOCAB_SIZE_TRG, )\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "try:\n",
    "    batch_ix = 0\n",
    "    while True:\n",
    "        loss_track = []\n",
    "        batches_gen = batchify_data(corpus_par, BATCH_SIZE)\n",
    "\n",
    "        for batch_data in batches_gen: \n",
    "            # split batch\n",
    "            batch_src, batch_trg = zip(*batch_data)\n",
    "\n",
    "            # prepare batch\n",
    "            enc_inp, enc_lengths = pad_data(batch_trg, append_suf=[2])\n",
    "            dec_inp, _ = pad_data(batch_trg, append_pre=[2])\n",
    "            dec_trg, dec_lengths = pad_data(batch_trg, append_suf=[2])\n",
    "            \n",
    "            if enc_inp.shape[1] > HARD_MAX_LEN:\n",
    "                print 'Skipping batch'\n",
    "                continue\n",
    "\n",
    "            # train\n",
    "            l, s, _ = model.train(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "            loss_track.append(l)\n",
    "\n",
    "            if batch_ix == 0 or batch_ix % 100 == 0:\n",
    "                print('=== BATCH {} ==='.format(batch_ix))\n",
    "                print('MINIBATCH LOSS: {}'.format(l))\n",
    "                l, s, p, _ = model.evaluate(sess, enc_inp, enc_lengths, dec_inp, dec_lengths, dec_trg)\n",
    "                for i, (inp, pred) in enumerate(zip(enc_inp, p)[:3]):\n",
    "                    print('sample {}:'.format(i + 1))\n",
    "                    print('input     >> {}'.format(' '.join([vocab_dec_b[word].encode('ascii', errors='replace') for word in inp])))\n",
    "                    print('predicted >> {}'.format(' '.join([vocab_dec_b[word].encode('ascii', errors='replace') for word in pred])))\n",
    "            batch_ix += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print 'Training Interrupted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define constant parameters\n",
    "DATA_SIZE = 500000\n",
    "VOCAB_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LENGTH = 5\n",
    "\n",
    "EMB_SIZE = 20\n",
    "ENC_HIDDEN_UNITS = 20\n",
    "DEC_HIDDEN_UNITS = ENC_HIDDEN_UNITS\n",
    "\n",
    "PAD = vocab_enc_a['<pad>']\n",
    "SOS = vocab_enc_a['<sos>']\n",
    "EOS = vocab_enc_a['<eos>']\n",
    "\n",
    "# define model inputs\n",
    "enc_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "dec_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "dec_targets = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "# define embedding matrix and lookup embeddings\n",
    "embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMB_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "enc_inputs_emb = tf.nn.embedding_lookup(embeddings, enc_inputs)\n",
    "dec_inputs_emb = tf.nn.embedding_lookup(embeddings, dec_inputs)\n",
    "\n",
    "# define encoder\n",
    "enc_cell = tf.contrib.rnn.GRUCell(ENC_HIDDEN_UNITS)\n",
    "_, enc_final_state = tf.nn.dynamic_rnn(\n",
    "    enc_cell, enc_inputs_emb, dtype=tf.float32, time_major=True, scope='encoder')\n",
    "\n",
    "# define decoder\n",
    "dec_cell = tf.contrib.rnn.GRUCell(DEC_HIDDEN_UNITS)\n",
    "dec_outputs, dec_final_state = tf.nn.dynamic_rnn(\n",
    "    dec_cell, dec_inputs_emb, initial_state=enc_final_state,\n",
    "    dtype=tf.float32, time_major=True, scope='decoder')\n",
    "\n",
    "# define output layer\n",
    "dec_logits = tf.contrib.layers.linear(dec_outputs, VOCAB_SIZE)\n",
    "dec_preds = tf.argmax(dec_logits, 2)\n",
    "\n",
    "# define loss function and optimizer\n",
    "step_centropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(dec_targets, depth=VOCAB_SIZE, dtype=tf.float32),\n",
    "    logits=dec_logits)\n",
    "loss = tf.reduce_mean(step_centropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "try:\n",
    "    batch_ix = 0\n",
    "    loss_track = []\n",
    "    batches_gen = batchify_data(toy_data_generator(VOCAB_SIZE, DATA_SIZE, MAX_SEQ_LENGTH), BATCH_SIZE)\n",
    "    \n",
    "    for data_batch in batches_gen: \n",
    "        enc_inc, lengths = pad_data(data_batch)\n",
    "        enc_len = lengths\n",
    "        dec_tar, _ = pad_data(data_batch, append_suf=[2, 0, 0])\n",
    "        \n",
    "        fd = {\n",
    "            enc_inputs: enc_inc.T,\n",
    "            enc_inputs_len: lengths,\n",
    "            dec_targets: dec_tar.T\n",
    "        }\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch_ix == 0 or batch_ix % 1000 == 0:\n",
    "            print('batch {}'.format(batch_ix))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(dec_preds, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[enc_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "        batch_ix += 1\n",
    "                    \n",
    "except KeyboardInterrupt:\n",
    "    print 'Training Interrupted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define constant parameters\n",
    "VOCAB_SIZE_SRC = len(vocab_a)\n",
    "VOCAB_SIZE_TRG = len(vocab_b)\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LENGTH = 5\n",
    "\n",
    "EMB_SIZE = 20\n",
    "ENC_HIDDEN_UNITS = 20\n",
    "DEC_HIDDEN_UNITS = ENC_HIDDEN_UNITS\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# define placeholders for data\n",
    "enc_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "enc_inputs_len = tf.placeholder(shape=(None), dtype=tf.int32, name='encoder_inputs_len')\n",
    "dec_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "dec_targets_len = tf.placeholder(shape=(None), dtype=tf.int32, name='decoder_targets_len')\n",
    "\n",
    "# define embeddings and lookup\n",
    "embeddings_src = tf.Variable(tf.random_uniform([VOCAB_SIZE_SRC, EMB_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "embeddings_trg = tf.Variable(tf.random_uniform([VOCAB_SIZE_TRG, EMB_SIZE], -1.0, 1.0), dtype=tf.float32)\n",
    "enc_inputs_emb = tf.nn.embedding_lookup(embeddings_src, enc_inputs)\n",
    "\n",
    "# define encoder\n",
    "enc_cell = tf.contrib.rnn.GRUCell(ENC_HIDDEN_UNITS)\n",
    "_, enc_final_state = tf.nn.dynamic_rnn(\n",
    "    enc_cell, enc_inputs_emb, dtype=tf.float32, time_major=True, scope='encoder_cell')\n",
    "# additional stuff must be done for attention\n",
    "\n",
    "# define decoder\n",
    "dec_cell = tf.contrib.rnn.GRUCell(DEC_HIDDEN_UNITS)\n",
    "# dec_lengths = enc_inputs_len + 3\n",
    "\n",
    "dec_smax_W = tf.Variable(tf.random_uniform([DEC_HIDDEN_UNITS, VOCAB_SIZE_TRG], -1.0, 1.0), dtype=tf.float32)\n",
    "dec_smax_b = tf.Variable(tf.zeros([VOCAB_SIZE_TRG]), dtype=tf.float32)\n",
    "\n",
    "eos_slice = tf.fill([BATCH_SIZE], vocab_enc_a['<eos>'], name='EOS')\n",
    "pad_slice = tf.fill([BATCH_SIZE], vocab_enc_a['<pad>'], name='PAD')\n",
    "\n",
    "eos_slice_emb = tf.nn.embedding_lookup(embeddings_src, eos_slice)\n",
    "pad_slice_emb = tf.nn.embedding_lookup(embeddings_src, pad_slice)\n",
    "\n",
    "# loop transition function (defines inputs of step t given outputs of step t-1)\n",
    "# (time, prev_cell_output, prev_cell_state, prev_loop_state) -> (elements_finished, input, cell_state, output, loop_state)\n",
    "def loop_fn(time, prev_output, prev_state, prev_loop_state):\n",
    "    if prev_state is None:\n",
    "        init_elements_finished = (0 >= dec_targets_len)\n",
    "        init_input = eos_slice_emb\n",
    "        init_cell_state = enc_final_state\n",
    "        init_cell_output = None\n",
    "        init_loop_state = None\n",
    "        return (init_elements_finished, init_input, init_cell_state, init_cell_output, init_loop_state)\n",
    "    else:\n",
    "        def get_next_input():\n",
    "            output_logits = tf.add(tf.matmul(prev_output, dec_smax_W), dec_smax_b)\n",
    "            pred = tf.argmax(output_logits, axis=1)\n",
    "            next_input = tf.nn.embedding_lookup(embeddings_trg, pred)\n",
    "            return next_input\n",
    "\n",
    "        step_elements_finished = (time >= dec_targets_len)\n",
    "        step_finished = tf.reduce_all(step_elements_finished)\n",
    "        step_input = tf.cond(step_finished, lambda: pad_slice_emb, get_next_input)\n",
    "        step_state = prev_state\n",
    "        step_output = prev_output\n",
    "        step_loop_state = None\n",
    "        return (step_elements_finished, step_input, step_state, step_output, step_loop_state)\n",
    "\n",
    "dec_outputs_ta, dec_final_state, _ = tf.nn.raw_rnn(\n",
    "    dec_cell, loop_fn)\n",
    "dec_outputs = dec_outputs_ta.stack()\n",
    "\n",
    "dec_max_steps, dec_batch_size, dec_dim = tf.unstack(tf.shape(dec_outputs))\n",
    "dec_outputs_flat = tf.reshape(dec_outputs, (-1, dec_dim))\n",
    "dec_logits_flat = tf.add(tf.matmul(dec_outputs_flat, dec_smax_W), dec_smax_b)\n",
    "dec_logits = tf.reshape(dec_logits_flat, (dec_max_steps, dec_batch_size, VOCAB_SIZE_TRG))\n",
    "dec_preds = tf.argmax(dec_logits, 2)\n",
    "\n",
    "# define loss function and optimizer\n",
    "stepwise_cent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(dec_targets, depth=VOCAB_SIZE_TRG, dtype=tf.float32),\n",
    "    logits=dec_logits)\n",
    "loss = tf.reduce_mean(stepwise_cent)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
