{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import scipy.sparse as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    \"\"\" Load corpus from text file and tokenize \"\"\"\n",
    "    corpus = []\n",
    "    vocab_cnt = Counter()\n",
    "    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    \n",
    "    with open(file_path) as fd:\n",
    "        for line in fd:\n",
    "            # clean lines from any punctuation characters\n",
    "            clean_line = re.sub('[\\+\\-\\.\\,\\:\\;\\\"\\?\\!\\>\\<\\=\\(\\)\\n]+', '', line)\n",
    "            tokens = tokenizer.tokenize(clean_line.lower())\n",
    "            corpus.append(tokens)\n",
    "            vocab_cnt.update(tokens)\n",
    "            \n",
    "    return corpus, vocab_cnt\n",
    "\n",
    "\n",
    "def code_tokens(vocab_cnt, corpus_size, min_occurrence=10, subsampling_t=1e-5, unk_symbol='<unk>'):\n",
    "    \"\"\" Filter vocabulary and encode tokens \"\"\"\n",
    "    vocab = ['<unk>']\n",
    "    \n",
    "    # filter vocabulary with min_occurrences and subsampling\n",
    "    for token, count in vocab_cnt.iteritems():\n",
    "        token_prob = max(1 - math.sqrt(subsampling_t / (count / float(corpus_size))), 0)\n",
    "        if count >= min_occurrence and np.random.rand() > token_prob:\n",
    "            vocab.append(token)\n",
    "    vocab_enc = {token: ix for ix, token in enumerate(vocab)}\n",
    "    vocab_dec = {ix: token for token, ix in vocab_enc.iteritems()}\n",
    "    \n",
    "    return vocab, vocab_enc, vocab_dec\n",
    "\n",
    "\n",
    "def generate_cooc_data(corpus, window_size=5, bidir_window=True):\n",
    "    \"\"\" Generate data with context in format (target, context, distance) \"\"\"\n",
    "    for center_ix, center_val in enumerate(corpus):\n",
    "        left_context = corpus[max(0, center_ix - window_size): center_ix]\n",
    "        left_context_len = len(left_context)\n",
    "        right_context = corpus[center_ix + 1: center_ix + window_size + 1]\n",
    "        right_context_len = len(right_context)\n",
    "\n",
    "        for context_ix, context_val in enumerate(left_context):\n",
    "            distance = left_context_len - context_ix\n",
    "            yield (center_val, context_val, distance)\n",
    "\n",
    "        if bidir_window:\n",
    "            for context_ix, context_val in enumerate(right_context):\n",
    "                distance = context_ix + 1\n",
    "                yield (center_val, context_val, distance)\n",
    "                    \n",
    "                    \n",
    "# WEIGHTING FUNCTIONS\n",
    "uniform_weight = lambda distance, length: 1.0\n",
    "# proposed by Pennington, used for GloVe\n",
    "harmonic_weight = lambda distance, length: 1.0 / distance\n",
    "# proposed by Mikolov, used for word2vec\n",
    "fraction_weight = lambda distance, length: float(distance) / length\n",
    "\n",
    "def build_cooc_matrix(data_generator, vocab_size, window_size, weight_fn=uniform_weight):\n",
    "    \"\"\" Build sparse cooccurrence matrix \"\"\"\n",
    "    cooc_matrix = ss.lil_matrix((vocab_size, vocab_size), dtype=np.float64)\n",
    "    \n",
    "    for target, context, distance in data_generator:\n",
    "        cooc_matrix[target, context] += weight_fn(distance, window_size)\n",
    "    return cooc_matrix.tocoo()\n",
    "\n",
    "\n",
    "def iterate_sparse_matrix(sparse_matrix):\n",
    "    \"\"\" Iterate a sparse COO matrix \"\"\"\n",
    "    return zip(sparse_matrix.row, sparse_matrix.col, sparse_matrix.data)\n",
    "\n",
    "\n",
    "def generate_context_data(corpus, max_window_size=5, skip_size=1, flatten=True):\n",
    "    \"\"\" Generate data with context in format (target, [contexts]) or (target, context) \"\"\"\n",
    "    for center_ix in xrange(max_window_size, len(corpus)-max_window_size, skip_size):\n",
    "        # sample a window size for the given center word\n",
    "        window_size = np.random.randint(max_window_size) + 1\n",
    "        full_context = corpus[center_ix-window_size:center_ix] + corpus[center_ix+1: center_ix+window_size+1]\n",
    "        \n",
    "        if flatten:\n",
    "            for context_ix in xrange(2*window_size):\n",
    "                yield (corpus[center_ix], full_context[context_ix])\n",
    "        else:\n",
    "            yield(corpus[center_ix], full_context)\n",
    "                \n",
    "                \n",
    "def batchify_data(data_generator, batch_size):\n",
    "    \"\"\" Split dataset (generator) into batches \"\"\"\n",
    "    if isinstance(data_generator, list):\n",
    "        for ix in xrange(0, len(data_generator), batch_size):\n",
    "            yield zip(*data_generator[ix:ix+batch_size])\n",
    "    else:\n",
    "        while data_generator:\n",
    "            buff = []\n",
    "            for ix in xrange(0, batch_size):\n",
    "                buff.append(next(data_generator))\n",
    "            yield zip(*buff)\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings_obj, file_name):\n",
    "    \"\"\" Save word embeddings and helper structures \"\"\"\n",
    "    with open(file_name, 'wb') as fd:\n",
    "        pickle.dump(embeddings_obj, fd)\n",
    "    \n",
    "\n",
    "def load_embeddings(file_name):\n",
    "    \"\"\" Load word embeddings and helper structures \"\"\"\n",
    "    with open(file_name, 'r') as fd:\n",
    "        embeddings_obj = pickle.load(fd)\n",
    "    return embeddings_obj\n",
    "    \n",
    "    \n",
    "def get_tsne_embeddings(embedding_matrix):\n",
    "    \"\"\" Compute t-SNE representation of embeddings \"\"\"\n",
    "    tsne = TSNE(perplexity=25, n_components=2, init='pca', n_iter=5000)\n",
    "    return tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "\n",
    "def get_pca_embeddings(embedding_matrix):\n",
    "    \"\"\" Compute PCA representation of embeddings \"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    return pca.fit_transform(embedding_matrix)\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings, words=[], words_cnt=500, method='pca', figsize=(8,8)):\n",
    "    \"\"\" Plot subset of embeddings in 2D space using t-SNE or PCA \"\"\"\n",
    "    embedding_matrix = embeddings._embeddings\n",
    "    vocab_dec = embeddings._vocab_dec\n",
    "    vocab_enc = embeddings._vocab_enc\n",
    "    \n",
    "    # prepare data\n",
    "    if not words:\n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        ixs = range(vocab_size)\n",
    "        random.shuffle(ixs)\n",
    "        chosen_ixs = ixs[:words_cnt]\n",
    "        labels = [vocab_dec[ix] for ix in chosen_ixs]\n",
    "        word_vecs = embedding_matrix[chosen_ixs]\n",
    "    else:\n",
    "        labels = words\n",
    "        chosen_ixs = [vocab_enc[word] for word in words]\n",
    "        word_vecs = embedding_matrix[chosen_ixs]\n",
    "        \n",
    "    if method == 'tsne':\n",
    "        low_dim_embeddings = get_tsne_embeddings(word_vecs)\n",
    "    else:\n",
    "        low_dim_embeddings = get_pca_embeddings(word_vecs)\n",
    "        \n",
    "    # plot reduced vectors\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for embedding, label in zip(low_dim_embeddings, labels):\n",
    "        x, y = embedding[0], embedding[1]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), \n",
    "                     textcoords='offset points', ha='right', \n",
    "                     va='bottom')\n",
    "    plt.yticks=[]\n",
    "    plt.xticks=[]\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "class Embeddings(object):\n",
    "    \"\"\" Class wrapping word embeddings \"\"\"\n",
    "    def __init__(self, embedding_matrix, vocab_enc, vocab_dec):\n",
    "        self._embeddings = embedding_matrix\n",
    "        self._vocab_enc = vocab_enc\n",
    "        self._vocab_dec = vocab_dec\n",
    "    \n",
    "    def find_embedding(self, word):\n",
    "        \"\"\" Find embedding for a given word \"\"\"\n",
    "        if isinstance(word, str):\n",
    "            word = self._vocab_enc[word]\n",
    "        return self._embeddings[word]\n",
    "    \n",
    "    def find_neighbors(self, word, k=5, nearest=True, exclude=[], include_scores=False):\n",
    "        \"\"\" Find neighboring words (semantic regularities) \"\"\"\n",
    "        word_ix = self._vocab_enc[word]\n",
    "        exclude = exclude + [word_ix]\n",
    "        \n",
    "        # find neighbors\n",
    "        word_emb = self._embeddings[word_ix]\n",
    "        similarities = self._embeddings.dot(word_emb)\n",
    "        similarities[exclude] = 0\n",
    "        best_matches = np.argsort(similarities)\n",
    "        trimmed_matches = best_matches[-k:][::-1] if nearest else best_matches[:k]\n",
    "        return [(self._vocab_dec[word_ix], similarities[word_ix]) for word_ix in trimmed_matches]\n",
    "    \n",
    "    def find_analogous(self, word_a, word_b, word_c, k=5):\n",
    "        \"\"\" Find analogous word (syntactic regularities: word_a - word_b = x - word_c) \"\"\"\n",
    "        word_a_ix, word_b_ix, word_c_ix = [self._vocab_enc[word] for word in [word_a, word_b, word_c]]\n",
    "        exclude = [word_a_ix, word_b_ix, word_c_ix]\n",
    "        \n",
    "        emb_a = self.find_embedding(word_a_ix) \n",
    "        emb_b = self.find_embedding(word_b_ix) \n",
    "        emb_c = self.find_embedding(word_c_ix) \n",
    "        emb_d_hat = emb_a - emb_b + emb_c\n",
    "        similarities = self._embeddings.dot(emb_d_hat)\n",
    "        similarities[exclude] = 0\n",
    "        best_matches = np.argsort(similarities)\n",
    "        trimmed_matches = best_matches[-k:][::-1]\n",
    "        return [(self._vocab_dec[word_ix], similarities[word_ix]) for word_ix in trimmed_matches]\n",
    "    \n",
    "    def vocab(self):\n",
    "        \"\"\" Return vocabulary list \"\"\"\n",
    "        return self._vocab_enc.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size (total tokens): 47\n",
      "Corpus vocabulary size (distinct tokens): 19\n",
      "Most popular words: [('graph', 5), ('trees', 5), ('i', 4), ('system', 4), ('user', 3)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6nKRMDOzuv4/1+k48+nZi1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103396ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TEXT_FILE = './corpora/trump-clean.txt'\n",
    "#TEXT_FILE = './corpora/presidential-clean.txt'\n",
    "#TEXT_FILE = './corpora/reviews-clean.txt'\n",
    "#TEXT_FILE = './corpora/reviews-clean-small.txt'\n",
    "#TEXT_FILE = './corpora/reviews-clean-xsmall.txt'\n",
    "TEXT_FILE = './corpora/amund.txt'\n",
    "\n",
    "# LOAD CORPUS AND SHOW STATS\n",
    "corpus, vocab_cnt = load_corpus(TEXT_FILE)\n",
    "raw_corpus_size = sum(vocab_cnt.itervalues())\n",
    "raw_vocab_size = len(vocab_cnt)\n",
    "\n",
    "print 'Corpus size (total tokens):', raw_corpus_size\n",
    "print 'Corpus vocabulary size (distinct tokens):', raw_vocab_size\n",
    "print 'Most popular words:', vocab_cnt.most_common(5)\n",
    "\n",
    "# visualize distribution\n",
    "counts = sorted(vocab_cnt.itervalues(), reverse=True)\n",
    "plt.semilogy(range(len(counts)), counts)\n",
    "plt.title('Distribution of token occurences')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Occurences')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean corpus size (w/o less freq words): 47\n",
      "Clean vocabulary size (w/o less freq words): 20\n"
     ]
    }
   ],
   "source": [
    "# FILTER LESS POPULAR TOKENS AND ENCODE TOKENS\n",
    "MIN_OCC = 0\n",
    "vocab, vocab_enc, vocab_dec = code_tokens(vocab_cnt, raw_corpus_size, min_occurrence=MIN_OCC, subsampling_t=1e-0)\n",
    "vocab_size = len(vocab)\n",
    "corpus_enc = [vocab_enc[word] for sentence in corpus for word in sentence if word in vocab_enc]\n",
    "#del corpus # for memory saving\n",
    "\n",
    "print 'Clean corpus size (w/o less freq words):', len(corpus_enc)\n",
    "print 'Clean vocabulary size (w/o less freq words):', vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    def __init__(self, embed_size, vocab_size, batch_size=128, num_neg_samples=64):\n",
    "        self._embed_size = embed_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._batch_size = batch_size\n",
    "        self._num_neg_samples = num_neg_samples\n",
    "        \n",
    "    def _setup_variables(self, learning_rate):\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        with self._graph.as_default():\n",
    "            # declare input/output placeholders\n",
    "            self._target_labels = tf.placeholder(tf.int32, shape=[self._batch_size], name='target_labels')\n",
    "            self._context_labels = tf.placeholder(tf.int32, shape=[self._batch_size, 1], name='context_labels')\n",
    "            \n",
    "            # declare weight/embedding matrices\n",
    "            embeddings = tf.Variable(tf.random_uniform([self._vocab_size, self._embed_size], 1.0, -1.0),\n",
    "                                       name='input_embeddings')\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([self._vocab_size, self._embed_size], stddev=1.0/math.sqrt(self._embed_size)), \n",
    "                                        name='softmax_weights')\n",
    "            softmax_b = tf.Variable(tf.zeros([self._vocab_size]), name='softmax_biases')\n",
    "            \n",
    "            \n",
    "            # lookup target/context embeddings\n",
    "            target_embeds = tf.nn.embedding_lookup(embeddings, self._target_labels)\n",
    "            \n",
    "            # skip-gram negative sampling loss\n",
    "            individual_losses = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, self._context_labels, \n",
    "                                                           target_embeds, self._num_neg_samples, \n",
    "                                                           self._vocab_size)\n",
    "            \n",
    "            # average loss in the batch\n",
    "            self._total_loss = tf.reduce_mean(individual_losses)\n",
    "            \n",
    "            # setup optimizer and logging\n",
    "            self._optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self._total_loss)\n",
    "            \n",
    "            # misc settings\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "            self._normalized_embeddings = embeddings / norm\n",
    "            \n",
    "            tf.summary.scalar(\"Skipgram_loss\", self._total_loss)\n",
    "            self._summary = tf.summary.merge_all()\n",
    "            \n",
    "    def train(self, data, max_epochs=25, learning_rate=0.05, window_size=5, skip_size=2, log_dir='./logs'):\n",
    "        print 'Initializing variables...'\n",
    "        self._setup_variables(learning_rate)\n",
    "        \n",
    "        training_step = 0\n",
    "        with tf.Session(graph=self._graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n",
    "            \n",
    "            for epoch in xrange(max_epochs):\n",
    "                data_generator = generate_context_data(data, window_size, skip_size)\n",
    "                data_batches = list(batchify_data(data_generator, self._batch_size))\n",
    "                random.shuffle(data_batches)\n",
    "                \n",
    "                epoch_loss, batch_count = 0, 0\n",
    "                for targets, contexts in tqdm(data_batches):\n",
    "                    if len(targets) != self._batch_size: continue\n",
    "                    \n",
    "                    targets = np.array(targets)\n",
    "                    contexts = np.array(contexts).reshape(-1, 1)\n",
    "                        \n",
    "                    feed_dict = {\n",
    "                        self._target_labels: targets,\n",
    "                        self._context_labels: contexts,\n",
    "                    }\n",
    "                    _, total_loss, summary_str = session.run([self._optimizer, self._total_loss, \n",
    "                                                              self._summary], feed_dict=feed_dict)\n",
    "                    \n",
    "                    if training_step % 5000 == 0: summary_writer.add_summary(summary_str, training_step)\n",
    "                        \n",
    "                    epoch_loss += total_loss\n",
    "                    batch_count += 1\n",
    "                    training_step += 1\n",
    "                tqdm.write('Epoch {} - avg loss: {}'.format(epoch, epoch_loss / batch_count))\n",
    "                self.word_embeddings = self._normalized_embeddings.eval()\n",
    "                \n",
    "            # combine and store word embeddings\n",
    "            self.word_embeddings = self._normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "SKIP_SIZE = 1\n",
    "MAX_EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "EMBED_SIZE = 50\n",
    "LEARNING_RATE = 1.0\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "skipgram = word2vec(EMBED_SIZE, VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
    "skipgram.train(corpus_enc, MAX_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_w2v = Embeddings(skipgram.word_embeddings, vocab_enc, vocab_dec)\n",
    "print emb_w2v.find_neighbors('sun', nearest=True)\n",
    "print emb_w2v.find_analogous('easiest', 'easy', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_embeddings(skipgram.word_embeddings, vocab_dec, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'w2v-airs-{}ep-{}lr-{}win-{}sk-{}bat-{}emb.em'.format(MAX_EPOCHS, LEARNING_RATE, WINDOW_SIZE, SKIP_SIZE, BATCH_SIZE, EMBED_SIZE)\n",
    "save_embeddings(emb_w2v, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in tqdm(xrange(5)): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(object):\n",
    "    def __init__(self, embed_size, vocab_size, batch_size=256, alpha_factor=0.75, occur_max=100, \n",
    "                 window_size=10, bidir_window=True):\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.alpha_factor = alpha_factor\n",
    "        self.occur_max = occur_max\n",
    "        self.window_size = window_size\n",
    "        self.bidir_window = True\n",
    "        \n",
    "    def _setup_variables(self, learning_rate):\n",
    "        self._graph = tf.Graph()\n",
    "        \n",
    "        with self._graph.as_default():\n",
    "            # declare model constants\n",
    "            # threshold of occurrences for the cost fn. weighting factor\n",
    "            occur_max = tf.constant([self.occur_max], shape=[self.batch_size], \n",
    "                                    dtype=tf.float32, name='max_occurrences')\n",
    "            # exponent of the cost fn. weighting factor\n",
    "            alpha_factor = tf.constant([self.alpha_factor], dtype=tf.float32,\n",
    "                                       name='alpha_factor')\n",
    "        \n",
    "            # declare input/output placeholders\n",
    "            self._target_words = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                              name='target_words')\n",
    "            self._context_words = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name='context_words')\n",
    "            self._cooc_counts = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                              name='cooccurrence_counts')\n",
    "            \n",
    "            # declare weight/embedding matrices\n",
    "            target_embeds = tf.Variable(tf.random_uniform([self.vocab_size, self.embed_size], 1.0, -1.0),\n",
    "                                       name='target_embeddings')\n",
    "            context_embeds = tf.Variable(tf.random_uniform([self.vocab_size, self.embed_size], 1.0, -1.0),\n",
    "                                       name='context_embeddings')\n",
    "            \n",
    "            target_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='target_biases')\n",
    "            context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='context_biases')\n",
    "            \n",
    "            # lookup target/context embeddings\n",
    "            target_embed = tf.nn.embedding_lookup([target_embeds], self._target_words)\n",
    "            target_bias = tf.nn.embedding_lookup([target_biases], self._target_words)\n",
    "            context_embed = tf.nn.embedding_lookup([context_embeds], self._context_words)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self._context_words)\n",
    "            \n",
    "            # glove algorithm computations\n",
    "            # weighting factor f(x) of the cost fn. balances out the negative effect of rare and popular words\n",
    "            weighting_factor = tf.minimum(1.0, tf.pow(tf.div(self._cooc_counts, occur_max), alpha_factor))\n",
    "            embedding_product = tf.reduce_sum(tf.multiply(target_embed, context_embed), 1)\n",
    "            # distance between embeddings including their biases and the log cooccurrence count\n",
    "            embedding_distance = tf.square(embedding_product + target_bias + context_bias - tf.log(self._cooc_counts))\n",
    "            \n",
    "            # losses for each element in the batch\n",
    "            individual_losses = tf.multiply(weighting_factor, embedding_distance)\n",
    "            # average loss in the batch\n",
    "            self._total_loss = tf.reduce_sum(individual_losses)\n",
    "            # combined word embeddings\n",
    "            self._combined_embeddings = tf.add(target_embeds, context_embeds, name='combined_embeddings')\n",
    "            \n",
    "            # setup optimizer and logging\n",
    "            self._optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self._total_loss)\n",
    "            \n",
    "            tf.summary.scalar(\"GloVe_loss\", self._total_loss)\n",
    "            self._summary = tf.summary.merge_all()\n",
    "            \n",
    "            \n",
    "    def train(self, cooc_matrix, max_epochs=25, init_learning_rate=0.05, log_dir='./logs'):\n",
    "        print 'Initializing variables...'\n",
    "        self._setup_variables(init_learning_rate)\n",
    "        print 'Preparing batched data...'\n",
    "        data_batches = list(batchify_data(iterate_sparse_matrix(cooc_matrix), self.batch_size))\n",
    "        \n",
    "        training_step = 0\n",
    "        with tf.Session(graph=self._graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n",
    "            \n",
    "            for epoch in xrange(max_epochs):\n",
    "                random.shuffle(data_batches)\n",
    "                \n",
    "                epoch_loss = 0\n",
    "                for targets, contexts, coocs in tqdm(data_batches):\n",
    "                    if len(targets) != self.batch_size: continue\n",
    "                        \n",
    "                    feed_dict = {\n",
    "                        self._target_words: targets,\n",
    "                        self._context_words: contexts,\n",
    "                        self._cooc_counts: coocs\n",
    "                    }\n",
    "                    _, total_loss, summary_str = session.run([self._optimizer, self._total_loss, self._summary], \n",
    "                                                             feed_dict=feed_dict)\n",
    "                    if training_step % 5000 == 0:\n",
    "                        summary_writer.add_summary(summary_str, training_step)\n",
    "                        \n",
    "                    epoch_loss += total_loss\n",
    "                    training_step += 1\n",
    "                print 'Epoch {} - avg loss: {}'.format(epoch, epoch_loss / len(data_batches))\n",
    "                embeddings = self._combined_embeddings.eval()\n",
    "                embeddings /= np.linalg.norm(embeddings, axis=1).reshape(-1, 1)\n",
    "                self.word_embeddings = embeddings\n",
    "                \n",
    "            # combine and store word embeddings\n",
    "            embeddings = self._combined_embeddings.eval()\n",
    "            embeddings /= np.linalg.norm(embeddings, axis=1).reshape(-1, 1)\n",
    "            self.word_embeddings = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cooccurrence data\n",
      "Generating cooccurrence matrix\n",
      "Initializing variables...\n",
      "Preparing batched data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6141.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1900.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3226.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4219.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2652.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3116.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1890.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3858.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5053.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2183.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3331.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - avg loss: 0.0\n",
      "Epoch 1 - avg loss: 0.0\n",
      "Epoch 2 - avg loss: 0.0\n",
      "Epoch 3 - avg loss: 0.0\n",
      "Epoch 4 - avg loss: 0.0\n",
      "Epoch 5 - avg loss: 0.0\n",
      "Epoch 6 - avg loss: 0.0\n",
      "Epoch 7 - avg loss: 0.0\n",
      "Epoch 8 - avg loss: 0.0\n",
      "Epoch 9 - avg loss: 0.0\n",
      "Epoch 10 - avg loss: 0.0\n",
      "Epoch 11 - avg loss: 0.0\n",
      "Epoch 12 - avg loss: 0.0\n",
      "Epoch 13 - avg loss: 0.0\n",
      "Epoch 14 - avg loss: 0.0\n",
      "Epoch 15 - avg loss: 0.0\n",
      "Epoch 16 - avg loss: 0.0\n",
      "Epoch 17 - avg loss: 0.0\n",
      "Epoch 18 - avg loss: 0.0\n",
      "Epoch 19 - avg loss: 0.0\n",
      "Epoch 20 - avg loss: 0.0\n",
      "Epoch 21 - avg loss: 0.0\n",
      "Epoch 22 - avg loss: 0.0\n",
      "Epoch 23 - avg loss: 0.0\n",
      "Epoch 24 - avg loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6326.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1685.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6710.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 395.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 679.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1795.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7358.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6615.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 707.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2281.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1589.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3731.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2888.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9619.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2449.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3597.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 805.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4424.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - avg loss: 0.0\n",
      "Epoch 26 - avg loss: 0.0\n",
      "Epoch 27 - avg loss: 0.0\n",
      "Epoch 28 - avg loss: 0.0\n",
      "Epoch 29 - avg loss: 0.0\n",
      "Epoch 30 - avg loss: 0.0\n",
      "Epoch 31 - avg loss: 0.0\n",
      "Epoch 32 - avg loss: 0.0\n",
      "Epoch 33 - avg loss: 0.0\n",
      "Epoch 34 - avg loss: 0.0\n",
      "Epoch 35 - avg loss: 0.0\n",
      "Epoch 36 - avg loss: 0.0\n",
      "Epoch 37 - avg loss: 0.0\n",
      "Epoch 38 - avg loss: 0.0\n",
      "Epoch 39 - avg loss: 0.0\n",
      "Epoch 40 - avg loss: 0.0\n",
      "Epoch 41 - avg loss: 0.0\n",
      "Epoch 42 - avg loss: 0.0\n",
      "Epoch 43 - avg loss: 0.0\n",
      "Epoch 44 - avg loss: 0.0\n",
      "Epoch 45 - avg loss: 0.0\n",
      "Epoch 46 - avg loss: 0.0\n",
      "Epoch 47 - avg loss: 0.0\n",
      "Epoch 48 - avg loss: 0.0\n",
      "Epoch 49 - avg loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "MAX_EPOCHS = 50\n",
    "EMBED_SIZE = 10\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "print 'Generating cooccurrence data'\n",
    "cooc_generator = generate_cooc_data(corpus_enc, bidir_window=False)\n",
    "print 'Generating cooccurrence matrix'\n",
    "cooc_matrix = build_cooc_matrix(cooc_generator, VOCAB_SIZE, WINDOW_SIZE, weight_fn=harmonic_weight)\n",
    "glove = GloVe(EMBED_SIZE, VOCAB_SIZE)\n",
    "glove.train(cooc_matrix, MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n",
      "[('system', 0.79635644), ('i', 0.5991568), ('<unk>', 0.59339547), ('trees', 0.58150041), ('survey', 0.43870816)]\n"
     ]
    }
   ],
   "source": [
    "emb_glove = Embeddings(glove.word_embeddings, vocab_enc, vocab_dec)\n",
    "print glove.word_embeddings.shape\n",
    "print emb_glove.find_neighbors('graph', nearest=True)\n",
    "#print emb_glove.find_analogous('best', 'good', 'easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_embeddings(glove.word_embeddings, vocab_dec, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'g_airxs-{}ep-{}lr-{}win-{}sk-{}bat-{}emb.em'.format(MAX_EPOCHS, LEARNING_RATE, WINDOW_SIZE, SKIP_SIZE, BATCH_SIZE, EMBED_SIZE)\n",
    "save_embeddings(emb_glove, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embeddings_path = './embeddings/g_airs-25ep-1.0lr-5win-1sk-256bat-50emb.em'\n",
    "#embeddings_path = './embeddings/g_airs-25ep-1.0lr-5win-1sk-256bat-100emb.em'\n",
    "#embeddings_path = './embeddings/w2v-airs-5ep-1.0lr-5win-1sk-256bat-50emb.em'\n",
    "embeddings_path = './embeddings/w2v-airs-5ep-1.0lr-5win-1sk-256bat-100emb.em'\n",
    "\n",
    "emb = load_embeddings(embeddings_path)\n",
    "#emb2 = Embeddings(emb._embeddings, emb._vocab_enc, emb._vocab_dec)\n",
    "#save_embeddings(emb2, embeddings_path)\n",
    "\n",
    "emb._embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vocab = ['bathroom', 'kitchen', 'bedroom', 'garage', 'garden', 'patio',\n",
    "             'roger', 'anne', 'emily', 'tom', 'david', 'caroline', 'dean', 'greg',\n",
    "             'diner', 'restaurant', 'bar', 'cafe', 'pub', 'club',\n",
    "             'cozy', 'clean', 'warm', 'pretty', 'welcoming', 'homelike',\n",
    "             'american', 'german', 'polish', 'spanish', 'irish', 'russian',\n",
    "             'berlin', 'seattle', 'london', 'amsterdam', 'miami',\n",
    "             'suffered', 'talked', 'round', 'worn', 'run', 'staples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_embeddings(emb, words=test_vocab, method='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_analogous('best', 'good', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_analogous('cleanest', 'clean', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_analogous('king', 'man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_neighbors('bathroom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_neighbors('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb.find_neighbors('view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
